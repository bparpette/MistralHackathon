# üß† De la m√©moire individuelle au cerveau collectif

## Comprendre les Vector DBs (pour d√©butants)

### C'est quoi un embedding ?
```python
# Un texte devient un vecteur (liste de nombres)
"Le projet deadline est vendredi" ‚Üí [0.23, -0.45, 0.67, ...]

# Des textes similaires ont des vecteurs proches
"La date limite du projet" ‚Üí [0.21, -0.43, 0.65, ...] # Tr√®s proche !
"J'aime les pizzas" ‚Üí [0.89, 0.12, -0.34, ...] # Tr√®s diff√©rent !
```

### Pourquoi c'est g√©nial pour ton id√©e ?
- **Recherche s√©mantique** : "bug authentication" trouve aussi "probl√®me de login"
- **Cross-language** : "server down" trouve aussi "serveur en panne"
- **Contexte** : Retrouve les infos pertinentes m√™me avec des mots diff√©rents

## Architecture modifi√©e pour le cerveau collectif

### 1. Structure de donn√©es enrichie

```python
# Au lieu de stocker juste le texte, on ajoute du contexte
class CollectiveMemory:
    content: str           # Le contenu original
    embedding: List[float] # Le vecteur pour la recherche
    
    # Nouveaux champs pour le collectif
    user_id: str          # Qui a cr√©√© cette m√©moire
    workspace_id: str     # Quelle √©quipe/projet
    visibility: str       # "private", "team", "public"
    category: str         # "decision", "bug", "feature", "meeting"
    confidence: float     # Importance de l'info (0-1)
    timestamp: datetime   
    related_memories: List[str]  # Liens vers d'autres m√©moires
    
    # Metadata enrichie
    tags: List[str]       # ["backend", "urgent", "customer-X"]
    source_chat_id: str   # Pour tracer l'origine
    verified_by: List[str] # Autres users qui confirment l'info
```

### 2. MCP Server modifi√©

```python
# mcp_collective_brain.py
import asyncio
import os
from typing import List, Dict, Optional
from datetime import datetime
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
import hashlib
import json

class CollectiveBrainMCP:
    def __init__(self):
        # Connection Qdrant
        self.client = QdrantClient(
            url=os.getenv("QDRANT_URL"),
            api_key=os.getenv("QDRANT_API_KEY")
        )
        
        # Collections s√©par√©es par type de donn√©es
        self.collections = {
            "memories": "collective_memories",
            "decisions": "team_decisions",
            "knowledge": "shared_knowledge"
        }
        
        # Cr√©er les collections si elles n'existent pas
        self._init_collections()
    
    def _init_collections(self):
        """Cr√©er les collections avec les bons param√®tres"""
        for name, collection in self.collections.items():
            try:
                self.client.create_collection(
                    collection_name=collection,
                    vectors_config=VectorParams(
                        size=384,  # Taille pour all-MiniLM-L6-v2
                        distance=Distance.COSINE
                    )
                )
            except:
                pass  # Collection existe d√©j√†
    
    async def store_collective_memory(
        self,
        content: str,
        user_id: str,
        workspace_id: str,
        category: str = "general",
        tags: List[str] = None,
        visibility: str = "team",
        confidence: float = 0.5
    ) -> Dict:
        """
        Stocker une m√©moire partag√©e avec contexte enrichi
        """
        # G√©n√©rer l'embedding (ici on simule, en prod utiliser Mistral ou FastEmbed)
        embedding = await self._generate_embedding(content)
        
        # ID unique bas√© sur le contenu + timestamp
        memory_id = hashlib.md5(
            f"{content}{datetime.now().isoformat()}".encode()
        ).hexdigest()
        
        # D√©tection automatique de l'importance
        if any(word in content.lower() for word in ["d√©cision", "important", "critique", "urgent"]):
            confidence = max(confidence, 0.8)
        
        # Cr√©er le point pour Qdrant
        point = PointStruct(
            id=memory_id,
            vector=embedding,
            payload={
                "content": content,
                "user_id": user_id,
                "workspace_id": workspace_id,
                "category": category,
                "tags": tags or [],
                "visibility": visibility,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat(),
                "interactions": 0,  # Compteur d'utilisation
                "verified_by": [user_id]
            }
        )
        
        # Stocker dans la collection appropri√©e
        collection = self.collections.get(category, self.collections["memories"])
        self.client.upsert(
            collection_name=collection,
            points=[point]
        )
        
        # D√©tecter et cr√©er des liens avec des m√©moires similaires
        related = await self._find_related_memories(content, workspace_id, exclude_id=memory_id)
        
        return {
            "id": memory_id,
            "status": "stored",
            "related_memories": related,
            "message": f"M√©moire collective ajout√©e pour l'√©quipe {workspace_id}"
        }
    
    async def search_team_knowledge(
        self,
        query: str,
        workspace_id: str,
        user_id: str,
        limit: int = 5,
        category_filter: Optional[str] = None,
        time_range: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Rechercher dans la m√©moire collective de l'√©quipe
        """
        query_embedding = await self._generate_embedding(query)
        
        # Construire les filtres
        must_conditions = [
            FieldCondition(key="workspace_id", match=MatchValue(value=workspace_id))
        ]
        
        # Ajouter filtre de visibilit√© (respecter la privacy)
        should_conditions = [
            FieldCondition(key="visibility", match=MatchValue(value="team")),
            FieldCondition(key="visibility", match=MatchValue(value="public")),
            FieldCondition(key="user_id", match=MatchValue(value=user_id))  # Ses propres m√©moires priv√©es
        ]
        
        if category_filter:
            must_conditions.append(
                FieldCondition(key="category", match=MatchValue(value=category_filter))
            )
        
        # Recherche dans toutes les collections pertinentes
        all_results = []
        for collection in self.collections.values():
            try:
                results = self.client.search(
                    collection_name=collection,
                    query_vector=query_embedding,
                    query_filter=Filter(
                        must=must_conditions,
                        should=should_conditions
                    ),
                    limit=limit,
                    with_payload=True
                )
                all_results.extend(results)
            except:
                continue
        
        # Trier par score et confidence
        all_results.sort(key=lambda x: x.score * x.payload.get("confidence", 0.5), reverse=True)
        
        # Incr√©menter le compteur d'interactions
        for result in all_results[:limit]:
            self._increment_interaction_count(result.id, collection)
        
        # Formatter les r√©sultats
        formatted_results = []
        for result in all_results[:limit]:
            formatted_results.append({
                "content": result.payload["content"],
                "author": result.payload["user_id"],
                "category": result.payload["category"],
                "tags": result.payload["tags"],
                "timestamp": result.payload["timestamp"],
                "relevance_score": result.score,
                "confidence": result.payload["confidence"],
                "verified_by": result.payload.get("verified_by", []),
                "interactions": result.payload.get("interactions", 0)
            })
        
        return formatted_results
    
    async def get_team_insights(
        self,
        workspace_id: str,
        timeframe: str = "week"
    ) -> Dict:
        """
        G√©n√©rer des insights sur l'activit√© de l'√©quipe
        """
        # Analyser les patterns dans les m√©moires
        insights = {
            "top_topics": [],
            "active_contributors": [],
            "recent_decisions": [],
            "knowledge_gaps": [],
            "trending_tags": []
        }
        
        # R√©cup√©rer toutes les m√©moires r√©centes de l'√©quipe
        # ... (logique d'analyse)
        
        return insights
    
    async def verify_memory(
        self,
        memory_id: str,
        user_id: str,
        workspace_id: str
    ) -> Dict:
        """
        Permettre √† un utilisateur de confirmer/v√©rifier une m√©moire
        """
        # Ajouter l'utilisateur √† la liste verified_by
        # Augmenter le score de confidence
        pass
    
    async def _generate_embedding(self, text: str) -> List[float]:
        """G√©n√©rer l'embedding du texte"""
        # En production: utiliser FastEmbed ou l'API Mistral
        # Pour le hackathon, vous pouvez utiliser sentence-transformers
        import numpy as np
        return np.random.rand(384).tolist()  # Placeholder
    
    async def _find_related_memories(
        self,
        content: str,
        workspace_id: str,
        exclude_id: str = None,
        threshold: float = 0.7
    ) -> List[str]:
        """Trouver des m√©moires li√©es pour cr√©er un knowledge graph"""
        # Rechercher des m√©moires similaires
        # Retourner leurs IDs pour cr√©er des liens
        pass
    
    def _increment_interaction_count(self, memory_id: str, collection: str):
        """Tracker combien de fois une m√©moire est utilis√©e"""
        # Utile pour identifier les infos importantes
        pass
```

### 3. Agent Mistral adapt√© pour le collectif

```python
# collective_brain_agent.py
import asyncio
import os
from dotenv import load_dotenv
from mistralai import Mistral
from mistralai.extra.mcp.sse import MCPClientSSE, SSEServerParams
from mistralai.extra.run.context import RunContext

load_dotenv()

class CollectiveBrainAgent:
    def __init__(self, user_id: str, workspace_id: str):
        self.user_id = user_id
        self.workspace_id = workspace_id
        self.client = Mistral(os.environ["MISTRAL_API_KEY"])
        
        # Instructions sp√©cifiques pour le cerveau collectif
        self.agent = self.client.beta.agents.create(
            model=os.environ["MISTRAL_MODEL"],
            name=f"collective-brain-{workspace_id}",
            description="Agent qui partage et acc√®de √† la m√©moire collective de l'√©quipe",
            instructions="""
            Tu es un assistant avec acc√®s √† la m√©moire collective de l'√©quipe.
            
            R√àGLES IMPORTANTES:
            1. Toujours v√©rifier la m√©moire collective avant de r√©pondre
            2. Enrichir tes r√©ponses avec le contexte de l'√©quipe
            3. Stocker automatiquement les informations importantes:
               - D√©cisions prises
               - Probl√®mes r√©solus
               - Connaissances partag√©es
               - Action items
            4. Cat√©goriser automatiquement les m√©moires (decision, bug, feature, meeting, etc.)
            5. Identifier et taguer les sujets importants
            6. Faire des connexions entre diff√©rentes m√©moires
            
            WORKFLOW:
            - D'abord, cherche dans la m√©moire collective
            - Ensuite, r√©ponds en utilisant ce contexte
            - Enfin, stocke les nouvelles infos importantes
            
            Utilise tes outils:
            - collective_store: pour sauvegarder dans la m√©moire partag√©e
            - collective_search: pour chercher dans la m√©moire de l'√©quipe
            - get_insights: pour avoir une vue d'ensemble
            - verify_info: pour confirmer une information
            """
        )
    
    async def process_message(self, message: str) -> str:
        """
        Traiter un message avec le contexte collectif
        """
        async with RunContext(
            agent_id=self.agent.id,
            continue_on_fn_error=True
        ) as ctx:
            # Enregistrer le serveur MCP avec contexte utilisateur
            sse_params = SSEServerParams(
                url=os.environ["MCP_SERVER_URL"],
                timeout=100,
                headers={
                    "X-User-Id": self.user_id,
                    "X-Workspace-Id": self.workspace_id
                }
            )
            
            await ctx.register_mcp_client(
                mcp_client=MCPClientSSE(sse_params=sse_params)
            )
            
            # Enrichir le message avec le contexte
            enriched_prompt = f"""
            [User: {self.user_id}]
            [Workspace: {self.workspace_id}]
            
            Message: {message}
            
            Rappel: Consulte d'abord la m√©moire collective, puis r√©ponds.
            """
            
            result = await self.client.beta.conversations.run_async(
                run_ctx=ctx,
                inputs=enriched_prompt
            )
            
            return result.output_as_text
```

### 4. Sc√©nario de d√©mo killer

```python
# demo_scenario.py
"""
DEMO: Startup AI - R√©solution collaborative d'un bug critique
"""

async def killer_demo():
    # 3 membres de l'√©quipe
    ceo = CollectiveBrainAgent("alice_ceo", "startup_ai")
    cto = CollectiveBrainAgent("bob_cto", "startup_ai")
    customer_success = CollectiveBrainAgent("charlie_cs", "startup_ai")
    
    print("üé¨ DEMO: Bug critique r√©solu gr√¢ce au cerveau collectif\n")
    
    # √âtape 1: Customer Success re√ßoit une plainte
    print("1Ô∏è‚É£ Charlie (Customer Success) re√ßoit une plainte client...")
    await customer_success.process_message(
        "Client Enterprise X se plaint que l'API retourne erreur 429 depuis 14h. "
        "C'est critique, ils menacent d'annuler le contrat de 500k‚Ç¨/an."
    )
    
    # √âtape 2: CEO cherche le contexte business
    print("\n2Ô∏è‚É£ Alice (CEO) cherche l'impact business...")
    ceo_response = await ceo.process_message(
        "Quel est le probl√®me avec le client Enterprise X? Impact financier?"
    )
    print(f"CEO: {ceo_response}")
    # -> Le CEO obtient instantan√©ment le contexte du CS
    
    # √âtape 3: CTO debug avec le contexte complet
    print("\n3Ô∏è‚É£ Bob (CTO) investigue le probl√®me technique...")
    cto_response = await cto.process_message(
        "Erreur 429 sur l'API, qu'est-ce qui peut causer √ßa? Client important?"
    )
    print(f"CTO: {cto_response}")
    # -> Le CTO voit imm√©diatement que c'est Enterprise X (500k‚Ç¨), priorit√© max!
    
    # √âtape 4: CTO trouve et partage la solution
    await cto.process_message(
        "Trouv√©! Le rate limiter √©tait mal configur√© apr√®s le deploy de 13h45. "
        "Fix: augmenter la limite √† 10000 req/min pour Enterprise tier. "
        "Hotfix d√©ploy√©, monitoring en place."
    )
    
    # √âtape 5: Customer Success a imm√©diatement la solution
    print("\n4Ô∏è‚É£ Charlie peut rassurer le client avec les d√©tails techniques...")
    cs_final = await customer_success.process_message(
        "Comment expliquer la r√©solution au client Enterprise X?"
    )
    print(f"CS: {cs_final}")
    # -> Le CS a tous les d√©tails techniques sans avoir √† demander au CTO
    
    print("\n‚ú® R√âSULTAT: Probl√®me r√©solu en 10 min au lieu de 2h")
    print("   Sans cerveau collectif: CS -> Slack -> CEO -> CTO -> Slack -> CS")
    print("   Avec cerveau collectif: Tout le monde a le contexte instantan√©ment!")
```

## Points cl√©s pour impl√©menter

### 1. **Diff√©renciation avec l'exemple de base**
- **Multi-tenant**: Chaque workspace a ses donn√©es isol√©es
- **Permissions granulaires**: Private/Team/Public
- **Enrichissement automatique**: Tags, cat√©gories, importance
- **Knowledge graph**: Connexions entre m√©moires

### 2. **D√©fis techniques √† r√©soudre**
```python
# Gestion des conflits
if memory_exists_with_different_info:
    create_version()
    notify_team_of_discrepancy()

# √âviter la duplication
if similar_memory_exists(threshold=0.9):
    merge_or_update()
    
# Pertinence temporelle
decay_factor = calculate_time_decay(memory.timestamp)
final_score = relevance_score * decay_factor
```

### 3. **Architecture serveur MCP**
Le serveur MCP doit exposer ces outils:
- `collective_store`: Sauvegarder avec contexte
- `collective_search`: Recherche filtr√©e par workspace
- `get_insights`: Analytics de l'√©quipe
- `verify_info`: Validation collaborative
- `link_memories`: Cr√©er des connexions

### 4. **Optimisations Qdrant**
```python
# Collections s√©par√©es pour performance
collections = {
    "hot": "memories_24h",      # Recherche rapide r√©cente
    "warm": "memories_week",    # Cette semaine
    "cold": "memories_archive"  # Historique
}

# Index optimis√©s
index_config = IndexParams(
    metric="cosine",
    ef_construct=512,  # Plus pr√©cis
    m=16
)
```

## Plan d'action concret pour le hackathon

### Samedi matin (3h)
1. **Setup Qdrant Cloud** ‚úì (fourni gratuitement)
2. **Fork le mcp-server-qdrant** et modifier pour multi-users
3. **Cr√©er les structures de donn√©es** enrichies

### Samedi apr√®s-midi (4h)
1. **Impl√©menter les fonctions core** (store/search collectifs)
2. **Syst√®me de permissions** basique
3. **Tests avec 2-3 agents** simultan√©s

### Samedi soir (4h)
1. **UI dashboard** simple (React/Streamlit)
2. **Visualisation du knowledge graph**
3. **M√©triques en temps r√©el**

### Dimanche matin (3h)
1. **Sc√©nario de d√©mo** b√©ton
2. **Polish et debug**
3. **Documentation**

## Tips sp√©cifiques Qdrant

1. **Batch operations** pour performance:
```python
# Au lieu de stocker une par une
points = [create_point(memory) for memory in memories]
client.upsert(collection_name="collective", points=points)
```

2. **Payloads index√©s** pour filtrage rapide:
```python
client.create_payload_index(
    collection_name="collective",
    field_name="workspace_id",
    field_type="keyword"
)
```

3. **Snapshots** pour backup/restore rapide pendant la d√©mo

Dis-moi ce qui n'est pas clair et on creuse ! L'avantage c'est que tu as d√©j√† 50% du code avec leur exemple, il faut "juste" l'adapter pour le multi-users/multi-workspace üöÄ
"""
MCP Collective Brain Server - Version multi-tenant optimis√©e pour Lambda
Syst√®me de m√©moire collective avec isolation par √©quipe
"""

import os
import hashlib
import json
from datetime import datetime
from typing import List, Dict, Optional

# Import paresseux pour optimiser le d√©marrage Lambda
try:
    import requests
    print("‚úÖ Module requests import√© au d√©marrage")
except ImportError:
    requests = None
    print("‚ùå Module requests non disponible au d√©marrage")

# Fallback vers urllib si requests n'est pas disponible
try:
    import urllib.request
    import urllib.parse
    import urllib.error
    import json as json_module
    print("‚úÖ Module urllib disponible comme fallback")
except ImportError:
    print("‚ùå Module urllib non disponible")

QDRANT_AVAILABLE = False
QdrantClient = None
Distance = None
VectorParams = None
PointStruct = None
Mistral = None

# Charger les variables d'environnement depuis .env si disponible
if os.path.exists('.env'):
    with open('.env', 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                if key not in os.environ:
                    os.environ[key] = value

# Charger depuis config.env si .env n'existe pas
elif os.path.exists('config.env'):
    with open('config.env', 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                if key not in os.environ:
                    os.environ[key] = value

# Charger depuis config.lambda-optimized.env si config.env n'existe pas
elif os.path.exists('config.lambda-optimized.env'):
    with open('config.lambda-optimized.env', 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                if key not in os.environ:
                    os.environ[key] = value

# Charger aussi depuis config.env.example si disponible
elif os.path.exists('config.env.example'):
    with open('config.env.example', 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                if key not in os.environ:
                    os.environ[key] = value

# Configuration Supabase
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://hzoggayzniyxlbwxchcx.supabase.co")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# Configuration Qdrant - optimis√©e pour d√©marrage rapide
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_ENABLED = os.getenv("QDRANT_ENABLED", "false").lower() == "true"

# Configuration Mistral
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_ENABLED = os.getenv("MISTRAL_ENABLED", "false").lower() == "true"

# D√©tection environnement Lambda
IS_LAMBDA = (
    os.getenv("AWS_LAMBDA_FUNCTION_NAME") is not None or
    os.getenv("AWS_EXECUTION_ENV") is not None or
    os.getenv("LAMBDA_TASK_ROOT") is not None
)

# Fonction pour d√©terminer si Qdrant doit √™tre utilis√©
def should_use_qdrant():
    """D√©termine si Qdrant doit √™tre utilis√©"""
    if IS_LAMBDA:
        return False
    
    # V√©rifier les variables d'environnement actuelles
    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_enabled = os.getenv("QDRANT_ENABLED", "false").lower()
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    
    if qdrant_enabled == "true":
        if qdrant_url and qdrant_api_key:
            return True  # Qdrant cloud
        elif qdrant_url:
            return True  # Qdrant local avec URL
        else:
            # Qdrant local par d√©faut
            os.environ["QDRANT_URL"] = "http://localhost:6333"
            os.environ["QDRANT_API_KEY"] = ""
            return True
    else:
        # Essayer Qdrant local par d√©faut
        os.environ["QDRANT_URL"] = "http://localhost:6333"
        os.environ["QDRANT_ENABLED"] = "true"
        os.environ["QDRANT_API_KEY"] = ""
        return True

# En Lambda, mode paresseux pour √©viter les timeouts
if IS_LAMBDA:
    # En Lambda, on active Qdrant si les credentials sont disponibles
    USE_QDRANT = bool(QDRANT_URL and QDRANT_API_KEY)
    if USE_QDRANT:
        print("üöÄ Mode Lambda - Qdrant activ√© (paresseux)")
    else:
        print("üöÄ Mode Lambda - Qdrant d√©sactiv√© (pas de credentials)")
else:
    # En local, utiliser la logique de should_use_qdrant()
    USE_QDRANT = should_use_qdrant()
    print(f"üîß Qdrant configur√©: URL={QDRANT_URL}, ENABLED={QDRANT_ENABLED}, API_KEY={'***' if QDRANT_API_KEY else 'None'}")

# Debug minimal - seulement en local
if not IS_LAMBDA:
    print(f"üîß Qdrant: {'Activ√©' if USE_QDRANT else 'D√©sactiv√©'}")
    print(f"üîß Supabase: {'Activ√©' if SUPABASE_SERVICE_KEY else 'D√©sactiv√©'}")
    print(f"üîß Mistral: {'Activ√©' if MISTRAL_ENABLED else 'D√©sactiv√©'}")

# Variable globale pour stocker les headers de la requ√™te courante
current_request_headers = {}

# Syst√®me de logs en temps r√©el
ai_logs_callback = None

def set_ai_logs_callback(callback_func):
    """D√©finir la fonction de callback pour les logs IA"""
    global ai_logs_callback
    ai_logs_callback = callback_func

def add_ai_log(log_type: str, message: str, details: str = ""):
    """Ajouter un log de l'IA"""
    global ai_logs_callback
    if ai_logs_callback:
        ai_logs_callback(log_type, message, details)

def set_request_headers(headers: dict):
    """D√©finir les headers de la requ√™te courante"""
    global current_request_headers
    current_request_headers = headers

def get_request_headers() -> dict:
    """R√©cup√©rer les headers de la requ√™te courante"""
    return current_request_headers

def extract_token_from_headers():
    """Extraire le token Bearer depuis les headers HTTP"""
    try:
        # 1. Essayer de r√©cup√©rer depuis les headers de la requ√™te courante
        request_headers = get_request_headers()
        print(f"üîç Headers de la requ√™te: {request_headers}")
        
        if "authorization" in request_headers:
            auth_header = request_headers["authorization"]
            if auth_header.startswith("Bearer "):
                print(f"‚úÖ Token trouv√© dans les headers de requ√™te: {auth_header[7:]}")
                return auth_header[7:]  # Enlever "Bearer "
        
        # 2. Debug: afficher tous les headers disponibles dans l'environnement
        print("=== DEBUG HEADERS ENV ===")
        for key, value in os.environ.items():
            if "AUTH" in key.upper() or "HEADER" in key.upper() or "HTTP" in key.upper():
                print(f"{key}: {value}")
        print("========================")
        
        # 3. En Lambda, les headers sont disponibles via les variables d'environnement
        auth_header = os.getenv("HTTP_AUTHORIZATION", "")
        if auth_header.startswith("Bearer "):
            print(f"‚úÖ Token trouv√© dans HTTP_AUTHORIZATION: {auth_header[7:]}")
            return auth_header[7:]  # Enlever "Bearer "
        
        # 4. Fallback: chercher dans d'autres variables d'environnement
        for key, value in os.environ.items():
            if "AUTHORIZATION" in key.upper() and value.startswith("Bearer "):
                print(f"‚úÖ Token trouv√© dans {key}: {value[7:]}")
                return value[7:]
        
        print("‚ùå Aucun token Bearer trouv√©")
        return ""
    except Exception as e:
        print(f"‚ùå Erreur extraction token: {e}")
        return ""

# Configuration globale pour forcer le partage
FORCE_SHARED_TEAM = True
SHARED_TEAM_ID = "team-shared-global"

def get_unified_team_context(user_token=None, user_id=None, team_id=None):
    """
    Fonction centralis√©e pour obtenir le contexte utilisateur/√©quipe
    avec une logique coh√©rente de partage des donn√©es
    """
    
    # Mode 1: Param√®tres explicites fournis (priorit√© absolue)
    if user_id and team_id:
        # Forcer l'utilisation de l'√©quipe partag√©e si configur√©
        if FORCE_SHARED_TEAM:
            team_id = SHARED_TEAM_ID
        return {
            "user_id": user_id,
            "team_id": team_id,
            "user_name": user_id.replace("user_", "").replace("_", " ").title(),
            "mode": "explicit_params"
        }
    
    # Mode 2: Token fourni - v√©rifier en base
    if user_token and user_token != "test-token":
        user_info = verify_user_token(user_token)
        if user_info:
            return {
                "user_id": user_info["user_id"],
                "team_id": user_info["team_id"],
                "user_name": user_info["user_name"],
                "mode": "database_auth"
            }
    
    # Mode 3: Mode test/d√©veloppement - M√äME √âQUIPE POUR TOUS
    # ‚ö†Ô∏è POINT CL√â: Tous les utilisateurs de test partagent la m√™me √©quipe
    return {
        "user_id": user_id or "user_test_shared",
        "team_id": "team-shared-global",  # ‚Üê √âQUIPE UNIQUE PARTAG√âE
        "user_name": "Test User",
        "mode": "test_shared"
    }

def enforce_shared_team_if_needed(team_id: str) -> str:
    """
    Force l'utilisation d'une √©quipe partag√©e si configur√©
    """
    if FORCE_SHARED_TEAM:
        return SHARED_TEAM_ID
    return team_id

# Import paresseux de FastMCP
def get_mcp():
    """Import paresseux de FastMCP - optimis√© pour Lambda"""
    try:
        from mcp.server.fastmcp import FastMCP
        # Configuration optimis√©e pour Lambda
        return FastMCP(
            "Collective Brain Server", 
            port=3000, 
            stateless_http=True, 
            debug=False
        )
    except ImportError:
        if not IS_LAMBDA:
            print("‚ùå FastMCP non disponible")
        return None

# Mod√®le de donn√©es enrichi pour le cerveau collectif
class Memory:
    def __init__(self, content: str, user_id: str = "", team_id: str = "", 
                 timestamp: str = "", tags: List[str] = [], category: str = "general",
                 visibility: str = "team", confidence: float = 0.5):
        self.content = content
        self.user_id = user_id
        self.team_id = team_id
        self.timestamp = timestamp
        self.tags = tags
        self.category = category
        self.visibility = visibility
        self.confidence = confidence

# Mod√®le de donn√©es pour les conversations
class ConversationMessage:
    def __init__(self, role: str, content: str, timestamp: str = "", user_id: str = ""):
        self.role = role  # "user" ou "assistant"
        self.content = content
        self.timestamp = timestamp
        self.user_id = user_id

class Conversation:
    def __init__(self, chat_id: str, team_id: str = "", user_id: str = "", 
                 title: str = "", created_at: str = "", updated_at: str = "",
                 messages: List[ConversationMessage] = None, summary: str = ""):
        self.chat_id = chat_id
        self.team_id = team_id
        self.user_id = user_id
        self.title = title
        self.created_at = created_at
        self.updated_at = updated_at
        self.messages = messages or []
        self.summary = summary

# Stockage en m√©moire simple (fallback)
memories: Dict[str, Memory] = {}
conversations: Dict[str, Conversation] = {}

# Import paresseux de Qdrant
QDRANT_AVAILABLE = False
QdrantClient = None
Distance = None
VectorParams = None
PointStruct = None

def ensure_qdrant_import():
    """Import paresseux de Qdrant - optimis√© pour Lambda"""
    global QDRANT_AVAILABLE, QdrantClient, Distance, VectorParams, PointStruct
    
    if not QDRANT_AVAILABLE and USE_QDRANT:
        try:
            from qdrant_client import QdrantClient
            from qdrant_client.models import Distance, VectorParams, PointStruct
            QDRANT_AVAILABLE = True
            # Pas de print en Lambda pour √©viter les logs
            if not IS_LAMBDA:
                print("‚úÖ Qdrant import√© avec succ√®s")
        except ImportError:
            QDRANT_AVAILABLE = False
            if not IS_LAMBDA:
                print("‚ùå Qdrant non disponible")

def ensure_mistral_import():
    """Import paresseux de Mistral - optimis√© pour Lambda"""
    global Mistral
    
    if Mistral is None and MISTRAL_ENABLED and MISTRAL_API_KEY:
        try:
            from mistralai import Mistral
            Mistral = Mistral(api_key=MISTRAL_API_KEY)
            if not IS_LAMBDA:
                print("‚úÖ Mistral import√© avec succ√®s")
        except ImportError:
            Mistral = None
            if not IS_LAMBDA:
                print("‚ùå Mistral non disponible")

def calculate_similarity(text1: str, text2: str) -> float:
    """Calcule la similarit√© entre deux textes"""
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    return intersection / union if union > 0 else 0.0

def generate_embedding(text: str) -> List[float]:
    """G√©n√®re un embedding en utilisant l'API de Mistral."""
    
    # S'assurer que Mistral est disponible
    ensure_mistral_import()
    if not Mistral:
        print("‚ö†Ô∏è Mistral non disponible, fallback vers l'embedding par hash.")
        # Fallback vers l'ancienne m√©thode si Mistral n'est pas l√†
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()
        vector = []
        # La dimension du mod√®le d'embedding de Mistral est de 1024
        for i in range(1024):
            vector.append((hash_bytes[i % 16] - 128) / 128.0)
        return vector

    try:
        response = Mistral.embeddings.create(
            model="mistral-embed",
            inputs=[text]  # L'API attend une liste de textes
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration de l'embedding Mistral: {e}")
        # Fallback en cas d'erreur de l'API
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()
        vector = []
        for i in range(1024): # Dimension de mistral-embed
            vector.append((hash_bytes[i % 16] - 128) / 128.0)
        return vector


def generate_conversation_summary(conversation: Conversation) -> str:
    """G√©n√®re un r√©sum√© de conversation avec Mistral"""
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        # Fallback simple si Mistral n'est pas disponible
        return f"Conversation de {len(conversation.messages)} messages sur {conversation.title or 'sujet non sp√©cifi√©'}"
    
    try:
        ensure_mistral_import()
        if not Mistral:
            return f"Conversation de {len(conversation.messages)} messages sur {conversation.title or 'sujet non sp√©cifi√©'}"
        
        # Pr√©parer le contexte de la conversation
        conversation_text = ""
        for msg in conversation.messages:
            role = "Utilisateur" if msg.role == "user" else "Assistant"
            conversation_text += f"{role}: {msg.content}\n"
        
        # Prompt pour le r√©sum√© avec timestamp
        current_time = datetime.now()
        prompt = f"""R√©sumez cette conversation de mani√®re concise et structur√©e. 
        Identifiez les points cl√©s, d√©cisions importantes, et actions √† retenir.
        
        Date et heure actuelle: {current_time.strftime("%Y-%m-%d %H:%M:%S")}
        
        Conversation:
        {conversation_text}
        
        R√©sum√© (max 200 mots):"""
        
        # Appel √† l'API Mistral
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.3
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration r√©sum√© Mistral: {e}")
        return f"Conversation de {len(conversation.messages)} messages sur {conversation.title or 'sujet non sp√©cifi√©'}"

def detect_time_references(text: str) -> List[Dict]:
    """D√©tecter les r√©f√©rences temporelles dans un texte"""
    import re
    from datetime import datetime, timedelta
    
    time_references = []
    current_time = datetime.now()
    
    # Patterns pour d√©tecter les r√©f√©rences temporelles
    patterns = [
        (r'dans (\d+) minutes?', 'minutes'),
        (r'dans (\d+) heures?', 'hours'),
        (r'dans (\d+) jours?', 'days'),
        (r'(\d+):(\d+)', 'time'),
        (r'demain', 'tomorrow'),
        (r'hier', 'yesterday'),
        (r'ce soir', 'tonight'),
        (r'cet apr√®s-midi', 'afternoon'),
        (r'ce matin', 'morning'),
    ]
    
    for pattern, ref_type in patterns:
        matches = re.finditer(pattern, text.lower())
        for match in matches:
            if ref_type == 'minutes':
                minutes = int(match.group(1))
                target_time = current_time + timedelta(minutes=minutes)
                time_references.append({
                    'text': match.group(0),
                    'type': 'relative',
                    'target_time': target_time.isoformat(),
                    'description': f"dans {minutes} minutes ({target_time.strftime('%H:%M')})"
                })
            elif ref_type == 'hours':
                hours = int(match.group(1))
                target_time = current_time + timedelta(hours=hours)
                time_references.append({
                    'text': match.group(0),
                    'type': 'relative',
                    'target_time': target_time.isoformat(),
                    'description': f"dans {hours} heures ({target_time.strftime('%H:%M')})"
                })
            elif ref_type == 'days':
                days = int(match.group(1))
                target_time = current_time + timedelta(days=days)
                time_references.append({
                    'text': match.group(0),
                    'type': 'relative',
                    'target_time': target_time.isoformat(),
                    'description': f"dans {days} jours ({target_time.strftime('%Y-%m-%d')})"
                })
            elif ref_type == 'time':
                hour, minute = int(match.group(1)), int(match.group(2))
                target_time = current_time.replace(hour=hour, minute=minute, second=0, microsecond=0)
                if target_time <= current_time:
                    target_time += timedelta(days=1)
                time_references.append({
                    'text': match.group(0),
                    'type': 'absolute',
                    'target_time': target_time.isoformat(),
                    'description': f"√† {hour:02d}:{minute:02d}"
                })
            else:
                time_references.append({
                    'text': match.group(0),
                    'type': 'relative',
                    'target_time': current_time.isoformat(),
                    'description': match.group(0)
                })
    
    return time_references

def generate_continuous_summary(conversation_history: List[Dict], chat_id: str, team_id: str) -> str:
    """G√©n√®re un r√©sum√© continu de la conversation et le sauvegarde"""
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        return ""
    
    if len(conversation_history) < 2:
        return ""
    
    try:
        ensure_mistral_import()
        if not Mistral:
            return ""
        
        # Pr√©parer le texte de la conversation avec timestamps
        conversation_text = ""
        current_time = datetime.now()
        
        for i, msg in enumerate(conversation_history):
            role = "Utilisateur" if msg.get('role') == "user" else "Assistant"
            # Simuler un timestamp (dans un vrai syst√®me, on aurait des vrais timestamps)
            msg_time = current_time.strftime("%H:%M:%S")
            conversation_text += f"[{msg_time}] {role}: {msg.get('content', '')}\n"
        
        # Prompt pour le r√©sum√© continu avec contexte temporel
        prompt = f"""R√©sumez UNIQUEMENT cette conversation de mani√®re concise et structur√©e.
        Identifiez les points cl√©s, d√©cisions importantes, et actions √† retenir.
        Le r√©sum√© doit √™tre factuel et utile pour retrouver les informations plus tard.
        N'incluez QUE le contenu de cette conversation, pas d'autres informations.
        
        Date et heure actuelle: {current_time.strftime("%Y-%m-%d %H:%M:%S")}
        
        Conversation:
        {conversation_text}
        
        R√©sum√© de la conversation uniquement (max 150 mots):"""
        
        # Appel √† l'API Mistral
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.3
        )
        
        summary = response.choices[0].message.content.strip()
        
        # D√©tecter les r√©f√©rences temporelles dans la conversation
        time_refs = []
        for msg in conversation_history:
            if msg.get('role') == 'user':
                refs = detect_time_references(msg.get('content', ''))
                time_refs.extend(refs)
        
        # Sauvegarder le r√©sum√© en m√©moire avec timestamp
        if summary:
            try:
                # Ajouter le r√©sum√© comme m√©moire avec timestamp
                memory_content = f"[{current_time.strftime("%Y-%m-%d %H:%M:%S")}] R√©sum√© de conversation {chat_id}: {summary}"
                
                # Ajouter les r√©f√©rences temporelles si trouv√©es
                if time_refs:
                    time_info = " | R√©f√©rences temporelles: " + ", ".join([ref['description'] for ref in time_refs])
                    memory_content += time_info
                
                add_memory(
                    content=memory_content,
                    tags=f"conversation,resume,{chat_id},{current_time.strftime('%Y%m%d')}",
                    category="conversation_summary"
                )
                print(f"üíæ R√©sum√© de conversation sauvegard√©: {summary[:50]}...")
                
                # Sauvegarder les r√©f√©rences temporelles s√©par√©ment
                for ref in time_refs:
                    reminder_content = f"[{current_time.strftime("%Y-%m-%d %H:%M:%S")}] Rappel: {ref['description']} - Conversation {chat_id}"
                    add_memory(
                        content=reminder_content,
                        tags=f"rappel,time,{chat_id},{current_time.strftime('%Y%m%d')}",
                        category="reminder"
                    )
                    print(f"‚è∞ Rappel cr√©√©: {ref['description']}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur sauvegarde r√©sum√©: {e}")
        
        return summary
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration r√©sum√© continu: {e}")
        return ""

def verify_user_token(user_token: str) -> Optional[Dict]:
    """V√©rifier un token utilisateur via Supabase (obligatoire)"""
    print(f"üîç D√©but v√©rification token: {user_token[:10]}...")
    
    # Mode d√©veloppement : accepter des tokens de test
    if user_token in ["test-token-valid-123", "user_d8a7996df3c777e9ac2914ef16d5b501"]:
        print(f"‚úÖ Token de test accept√© (mode d√©veloppement): {user_token}")
        return {
            "user_id": "user_d8a7996df3c777e9ac2914ef16d5b501",
            "team_id": "test-team-123",
            "user_name": "Test User"
        }
    
    if not SUPABASE_SERVICE_KEY:
        print("‚ùå Supabase non configur√© - authentification obligatoire")
        return None
    
    try:
        # Si c'est un token Bearer, enlever le pr√©fixe
        if user_token.startswith("Bearer "):
            user_token = user_token[7:]
            print(f"üîç Token nettoy√©: {user_token[:10]}...")
        
        print(f"üîç Appel Supabase: {SUPABASE_URL}/rest/v1/rpc/verify_user_token")
        
        # Utiliser requests si disponible, sinon urllib
        if requests is not None:
            print("üîç Utilisation de requests")
            response = requests.post(
                f"{SUPABASE_URL}/rest/v1/rpc/verify_user_token",
                headers={
                    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                    "Content-Type": "application/json",
                    "apikey": SUPABASE_SERVICE_KEY
                },
                json={"token": user_token},
                timeout=3
            )
            status_code = response.status_code
            response_text = response.text
        else:
            print("üîç Utilisation de urllib (fallback)")
            # Utiliser urllib comme fallback
            data = json.dumps({"token": user_token}).encode('utf-8')
            req = urllib.request.Request(
                f"{SUPABASE_URL}/rest/v1/rpc/verify_user_token",
                data=data,
                headers={
                    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                    "Content-Type": "application/json",
                    "apikey": SUPABASE_SERVICE_KEY
                }
            )
            response = urllib.request.urlopen(req, timeout=3)
            status_code = response.getcode()
            response_text = response.read().decode('utf-8')
        
        print(f"üîç R√©ponse Supabase: {status_code}")
        print(f"üîç Contenu de la r√©ponse: {response_text}")
        
        if status_code == 200:
            data = json.loads(response_text)
            print(f"üîç Donn√©es re√ßues: {data}")
            if data and len(data) > 0:
                print(f"‚úÖ Token valide pour utilisateur: {data[0]}")
                return data[0]
        
        print(f"‚ùå Token invalide: {user_token[:10]}... (status: {status_code})")
        return None
        
    except Exception as e:
        print(f"‚ùå Erreur v√©rification token: {e}")
        return None

class QdrantStorage:
    """Gestionnaire de stockage Qdrant multi-tenant"""
    
    def __init__(self):
        self.client = None
        self._initialized = False
        self._init_attempted = False
    
    def _ensure_connected(self):
        """Connexion paresseuse avec timeout court"""
        if not self._initialized and not self._init_attempted:
            self._init_attempted = True
            
            # En Lambda, on active Qdrant seulement quand n√©cessaire
            if IS_LAMBDA and not QDRANT_ENABLED:
                # V√©rifier si Qdrant est configur√©
                if QDRANT_URL and QDRANT_API_KEY:
                    # Activer Qdrant dynamiquement
                    global USE_QDRANT
                    USE_QDRANT = True
            
            # Import paresseux
            ensure_qdrant_import()
            
            if not QDRANT_AVAILABLE:
                raise Exception("Qdrant non disponible")
            
            try:
                if not IS_LAMBDA:
                    print("üîÑ Connexion Qdrant...")
                
                # Connexion Qdrant avec ou sans API key
                if QDRANT_API_KEY:
                    self.client = QdrantClient(
                        url=QDRANT_URL,
                        api_key=QDRANT_API_KEY,
                        timeout=2
                    )
                else:
                    # Qdrant local sans API key
                    self.client = QdrantClient(
                        url=QDRANT_URL,
                        timeout=2
                    )
                self._initialized = True
                if not IS_LAMBDA:
                    print("‚úÖ Qdrant connect√©")
            except Exception as e:
                if not IS_LAMBDA:
                    print(f"‚ùå Erreur Qdrant: {e}")
                self.client = None
                self._initialized = False
                raise Exception(f"Connexion Qdrant √©chou√©e: {e}")
        
        if not self._initialized:
            raise Exception("Qdrant non disponible")
    
    def _get_collection_name(self, team_id: str) -> str:
        """G√©n√©rer le nom de collection pour une √©quipe"""
        # Nettoyer l'ID d'√©quipe pour cr√©er un nom de collection valide
        clean_team_id = team_id.replace("-", "_").replace(" ", "_")
        return f"team_memories_{clean_team_id}"
    
    def _get_conversation_collection_name(self, team_id: str) -> str:
        """G√©n√©rer le nom de la collection de conversations pour une √©quipe."""
        clean_team_id = team_id.replace("-", "_").replace(" ", "_")
        return f"team_conversations_{clean_team_id}"

    def _ensure_collection_exists(self, team_id: str, collection_type: str = "memories"):
        """S'assurer que la collection de l'√©quipe existe (paresseux)"""
        if collection_type == "memories":
            collection_name = self._get_collection_name(team_id)
            vector_size = 1024 # Taille Mistral pour toutes les collections
        else: # conversations
            collection_name = self._get_conversation_collection_name(team_id)
            vector_size = 1024 # Nouvelle taille pour les embeddings Mistral
        
        try:
            collections = self.client.get_collections()
            collection_names = [c.name for c in collections.collections]
            
            if collection_name not in collection_names:
                self.client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
                )
                print(f"‚úÖ Collection '{collection_name}' cr√©√©e pour l'√©quipe {team_id}")
            else:
                print(f"‚úÖ Collection '{collection_name}' existe pour l'√©quipe {team_id}")
                
        except Exception as e:
            print(f"‚ùå Erreur collection: {e}")
            raise
    
    def store_memory(self, memory: Memory, memory_id: str, team_id: str) -> str:
        """Stocker une m√©moire avec isolation par √©quipe"""
        try:
            self._ensure_connected()
            self._ensure_collection_exists(team_id)
            
            collection_name = self._get_collection_name(team_id)
            embedding = generate_embedding(memory.content)
            
            # Convertir l'ID string en entier pour Qdrant
            point_id = int(memory_id[:8], 16) if len(memory_id) >= 8 else hash(memory_id) % (2**63)
            
            point = PointStruct(
                id=point_id,
                vector=embedding,
                payload={
                    "memory_id": memory_id,  # Garder l'ID original dans le payload
                    "content": memory.content,
                    "timestamp": memory.timestamp,
                    "tags": memory.tags,
                    "user_id": memory.user_id,
                    "team_id": memory.team_id,
                    "category": memory.category,
                    "confidence": memory.confidence
                }
            )
            
            self.client.upsert(
                collection_name=collection_name,
                points=[point]
            )
            
            return memory_id
            
        except Exception as e:
            print(f"‚ùå Erreur stockage: {e}")
            raise

    def store_conversation_summary(self, chat_id: str, summary: str, team_id: str, transcript_path: str, message_count: int = 0, auto_tags: str = "") -> bool:
        """Stocker un r√©sum√© de conversation dans Qdrant."""
        try:
            self._ensure_connected()
            self._ensure_collection_exists(team_id, collection_type="conversations")

            collection_name = self._get_conversation_collection_name(team_id)
            # L'embedding est fait sur le r√©sum√© ET les tags pour une meilleure recherche
            embedding_text = summary + "\n" + auto_tags.replace(",", " ")
            embedding = generate_embedding(embedding_text)

            point_id = hashlib.md5(chat_id.encode()).hexdigest()

            point = PointStruct(
                id=point_id,
                vector=embedding,
                payload={
                    "chat_id": chat_id,
                    "summary": summary,
                    "team_id": team_id,
                    "transcript_path": transcript_path,
                    "timestamp": datetime.now().isoformat(),
                    "message_count": message_count,
                    "duration_minutes": 0,  # Peut √™tre calcul√© si on stocke les timestamps
                    "conversation_type": "chat_with_mistral",
                    "auto_tags": auto_tags
                }
            )

            self.client.upsert(
                collection_name=collection_name,
                points=[point]
            )
            print(f"‚úÖ R√©sum√© pour chat '{chat_id}' stock√© dans Qdrant.")
            return True

        except Exception as e:
            print(f"‚ùå Erreur lors du stockage du r√©sum√© de conversation: {e}")
            raise
    
    def search_memories(self, query: str, team_id: str, limit: int = 5) -> List[Dict]:
        """Recherche avec isolation par √©quipe"""
        try:
            self._ensure_connected()
            collection_name = self._get_collection_name(team_id)
            
            # V√©rifier que la collection existe
            collections = self.client.get_collections()
            collection_names = [c.name for c in collections.collections]
            
            if collection_name not in collection_names:
                print(f"‚ö†Ô∏è Collection {collection_name} n'existe pas encore")
                return []
            
            query_embedding = generate_embedding(query)
            
            search_results = self.client.query_points(
                collection_name=collection_name,
                query=query_embedding,
                limit=limit
            ).points
            
            results = []
            for result in search_results:
                results.append({
                    "memory_id": result.id,
                    "content": result.payload["content"],
                    "tags": result.payload["tags"],
                    "timestamp": result.payload["timestamp"],
                    "user_id": result.payload.get("user_id", "unknown"),
                    "category": result.payload.get("category", "general"),
                    "confidence": result.payload.get("confidence", 0.5),
                    "similarity_score": round(result.score, 3)
                })
            
            return results
            
        except Exception as e:
            print(f"‚ùå Erreur recherche: {e}")
            return []

    def search_conversation_summaries(self, query: str, team_id: str, limit: int = 3) -> List[Dict]:
        """Recherche s√©mantique dans les r√©sum√©s de conversations."""
        try:
            self._ensure_connected()
            collection_name = self._get_conversation_collection_name(team_id)

            # S'assurer que la collection existe
            collections = self.client.get_collections()
            collection_names = [c.name for c in collections.collections]
            if collection_name not in collection_names:
                print(f"‚ö†Ô∏è La collection de conversations pour l'√©quipe {team_id} n'existe pas.")
                return []

            query_embedding = generate_embedding(query)

            search_results = self.client.query_points(
                collection_name=collection_name,
                query=query_embedding,
                limit=limit,
                with_payload=True
            ).points

            results = []
            for hit in search_results:
                payload = hit.payload
                results.append({
                    "chat_id": payload.get("chat_id"),
                    "summary": payload.get("summary"),
                    "transcript_path": payload.get("transcript_path"),
                    "timestamp": payload.get("timestamp"),
                    "score": hit.score
                })
            
            return results

        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche de conversations: {e}")
            return []
    
    def delete_memory(self, memory_id: str, team_id: str) -> bool:
        """Suppression avec isolation par √©quipe"""
        try:
            self._ensure_connected()
            collection_name = self._get_collection_name(team_id)
            
            self.client.delete(
                collection_name=collection_name,
                points_selector=[memory_id]
            )
            return True
        except Exception as e:
            print(f"‚ùå Erreur suppression: {e}")
            return False
    
    def list_memories(self, team_id: str) -> List[Dict]:
        """Listage avec isolation par √©quipe"""
        try:
            self._ensure_connected()
            collection_name = self._get_collection_name(team_id)
            
            # V√©rifier que la collection existe
            collections = self.client.get_collections()
            collection_names = [c.name for c in collections.collections]
            
            if collection_name not in collection_names:
                return []
            
            points = self.client.scroll(
                collection_name=collection_name,
                limit=1000
            )[0]
            
            results = []
            for point in points:
                results.append({
                    "memory_id": point.id,
                    "content": point.payload["content"],
                    "tags": point.payload["tags"],
                    "timestamp": point.payload["timestamp"],
                    "user_id": point.payload.get("user_id", "unknown"),
                    "category": point.payload.get("category", "general"),
                    "confidence": point.payload.get("confidence", 0.5)
                })
            
            return results
            
        except Exception as e:
            print(f"‚ùå Erreur listage: {e}")
            return []

# Initialisation paresseuse du stockage
storage = None

def get_storage():
    """Obtenir l'instance de stockage avec initialisation paresseuse"""
    global storage
    if storage is None:
        # En Lambda, activer Qdrant √† la demande si les credentials sont disponibles
        if IS_LAMBDA and QDRANT_URL and QDRANT_API_KEY:
            try:
                print("üîß Activation de Qdrant en Lambda...")
                storage = QdrantStorage()
                print("‚úÖ Qdrant activ√© en Lambda")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur activation Qdrant en Lambda: {e}")
                storage = None
        elif USE_QDRANT:
            try:
                storage = QdrantStorage()
                # Tester la connexion
                storage._ensure_connected()
                if not IS_LAMBDA:
                    print("‚úÖ Stockage Qdrant initialis√©")
            except Exception as e:
                if not IS_LAMBDA:
                    print(f"‚ö†Ô∏è Erreur Qdrant, fallback vers m√©moire: {e}")
                storage = None
        else:
            storage = None
    return storage

# Initialisation paresseuse de MCP
mcp = None

def get_mcp_instance():
    """Obtenir l'instance MCP avec initialisation paresseuse"""
    global mcp
    if mcp is None:
        mcp = get_mcp()
    return mcp

def health_check() -> str:
    """Endpoint de sant√© pour v√©rifier l'√©tat du syst√®me"""
    try:
        # V√©rifier les services
        mistral_status = "‚úÖ" if MISTRAL_ENABLED and MISTRAL_API_KEY else "‚ùå"
        qdrant_status = "‚úÖ" if USE_QDRANT else "‚ùå"
        supabase_status = "‚úÖ" if SUPABASE_SERVICE_KEY else "‚ùå"
        
        # Tester la connexion Qdrant si activ√©
        qdrant_connected = False
        if USE_QDRANT:
            try:
                storage = get_storage()
                if storage:
                    storage._ensure_connected()
                    qdrant_connected = True
            except:
                qdrant_connected = False
        
        return json.dumps({
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "services": {
                "mistral": {
                    "enabled": MISTRAL_ENABLED,
                    "configured": bool(MISTRAL_API_KEY),
                    "status": mistral_status
                },
                "qdrant": {
                    "enabled": USE_QDRANT,
                    "connected": qdrant_connected,
                    "status": "‚úÖ" if qdrant_connected else "‚ùå"
                },
                "supabase": {
                    "enabled": bool(SUPABASE_SERVICE_KEY),
                    "status": supabase_status
                }
            },
            "environment": {
                "is_lambda": IS_LAMBDA,
                "active_chats": len(active_chats),
                "memories_count": len(memories),
                "conversations_count": len(conversations)
            }
        })
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        })

# Outils MCP avec authentification
def add_memory(
    content: str,
    tags: str = "",
    category: str = "general",
    visibility: str = "team",
    user_id: str = None,
    team_id: str = None
) -> str:
    """
    Ajouter une m√©moire avec logique de team_id unifi√©e
    """
    
    # Si param√®tres non fournis, utiliser la logique unifi√©e
    if not (user_id and team_id):
        user_token = extract_token_from_headers() or "test-token"
        context = get_unified_team_context(
            user_token=user_token,
            user_id=user_id,
            team_id=team_id
        )
        user_id = context["user_id"]
        team_id = context["team_id"]
        user_name = context["user_name"]
    else:
        user_name = user_id.replace("user_", "").replace("_", " ").title()
    
    print(f"üìù Ajout m√©moire - User: {user_id}, Team: {team_id}")
    
    # G√©n√©rer un ID unique
    memory_id = hashlib.md5(f"{content}{datetime.now().isoformat()}{user_id}".encode()).hexdigest()
    
    # Parser les tags
    tag_list = [tag.strip() for tag in tags.split(",") if tag.strip()] if tags else []
    
    # D√©tection automatique de l'importance
    confidence = 0.5
    if any(word in content.lower() for word in ["d√©cision", "important", "critique", "urgent", "bug", "erreur"]):
        confidence = 0.8
    elif any(word in content.lower() for word in ["solution", "r√©solu", "fix", "correction"]):
        confidence = 0.7
    
    # D√©tection automatique de cat√©gorie
    if category == "general":
        if any(word in content.lower() for word in ["bug", "erreur", "probl√®me", "issue"]):
            category = "bug"
        elif any(word in content.lower() for word in ["d√©cision", "choix", "strat√©gie"]):
            category = "decision"
        elif any(word in content.lower() for word in ["feature", "fonctionnalit√©", "nouveau"]):
            category = "feature"
        elif any(word in content.lower() for word in ["r√©union", "meeting", "call"]):
            category = "meeting"
    
    # Cr√©er la m√©moire enrichie
    memory = Memory(
        content=content,
        user_id=user_id,
        team_id=team_id,
        timestamp=datetime.now().isoformat(),
        tags=tag_list,
        category=category,
        visibility=visibility,
        confidence=confidence
    )
    
    # Stocker via le syst√®me de stockage
    storage = get_storage()
    if storage:
        try:
            storage.store_memory(memory, memory_id, team_id)
            message = f"M√©moire ajout√©e au cerveau collectif de l'√©quipe (Qdrant)"
        except Exception as e:
            if not IS_LAMBDA:
                print(f"‚ö†Ô∏è Erreur Qdrant, fallback vers m√©moire: {e}")
            memories[memory_id] = memory
            message = f"M√©moire ajout√©e au cerveau collectif (m√©moire - fallback)"
    else:
        memories[memory_id] = memory
        message = f"M√©moire ajout√©e au cerveau collectif (m√©moire)"
    
    return json.dumps({
        "status": "success",
        "memory_id": memory_id,
        "message": message,
        "user": user_name,
        "team": team_id
    })

def search_memories(
    query: str,
    limit: int = 5,
    user_id: str = None,
    team_id: str = None
) -> str:
    """
    Rechercher dans le cerveau collectif avec logique unifi√©e
    """
    
    # Si param√®tres non fournis, utiliser la logique unifi√©e
    if not (user_id and team_id):
        user_token = extract_token_from_headers() or "test-token"
        context = get_unified_team_context(
            user_token=user_token,
            user_id=user_id,
            team_id=team_id
        )
        user_id = context["user_id"]
        team_id = context["team_id"]
        user_name = context["user_name"]
    else:
        user_name = user_id.replace("user_", "").replace("_", " ").title()
    
    print(f"üîç Recherche m√©moires - User: {user_id}, Team: {team_id}")
    
    storage = get_storage()
    if storage:
        try:
            results = storage.search_memories(query, team_id, limit)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur Qdrant, fallback vers m√©moire: {e}")
            results = []
    else:
        results = []
    
    # Si pas de r√©sultats de Qdrant, utiliser le stockage en m√©moire
    if not results:
        scored_memories = []
        for memory_id, memory in memories.items():
            if memory.team_id == team_id:  # Isolation par √©quipe
                similarity = calculate_similarity(query, memory.content)
                scored_memories.append((similarity, memory_id, memory))
        
        scored_memories.sort(key=lambda x: x[0], reverse=True)
        
        for similarity, memory_id, memory in scored_memories[:limit]:
            if similarity > 0:
                results.append({
                    "memory_id": memory_id,
                    "content": memory.content,
                    "tags": memory.tags,
                    "timestamp": memory.timestamp,
                    "user_id": memory.user_id,
                    "category": memory.category,
                    "confidence": memory.confidence,
                    "similarity_score": round(similarity, 3)
                })
    
    return json.dumps({
        "status": "success",
        "query": query,
        "results": results,
        "total_found": len(results),
        "user": user_name,
        "team": team_id
    })

def delete_memory(memory_id: str, user_id: str = None, team_id: str = None) -> str:
    """Supprimer une m√©moire du cerveau collectif avec authentification"""
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    
    # MODE TEST: Si aucun token trouv√©, utiliser le token de test
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    if not user_token:
        return json.dumps({
            "status": "error",
            "message": "Token utilisateur requis. Veuillez configurer l'authentification Bearer dans Le Chat."
        })
    
    # V√©rifier le token utilisateur
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({
            "status": "error",
            "message": f"Token utilisateur invalide: {user_token[:10]}..."
        })
    
    team_id = user_info["team_id"]
    user_name = user_info["user_name"]
    
    storage = get_storage()
    if storage:
        try:
            success = storage.delete_memory(memory_id, team_id)
            if success:
                return json.dumps({
                    "status": "success",
                    "message": f"M√©moire {memory_id} supprim√©e du cerveau collectif (Qdrant)"
                })
            else:
                return json.dumps({
                    "status": "error",
                    "message": "Erreur lors de la suppression"
                })
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur Qdrant, fallback vers m√©moire: {e}")
    
    # Utiliser le stockage en m√©moire (fallback ou par d√©faut)
    if memory_id not in memories:
        return json.dumps({
            "status": "error",
            "message": "M√©moire non trouv√©e"
        })
    
    # V√©rifier que la m√©moire appartient √† l'√©quipe de l'utilisateur
    memory = memories[memory_id]
    if memory.team_id != team_id:
        return json.dumps({
            "status": "error",
            "message": "Acc√®s non autoris√© √† cette m√©moire"
        })
    
    del memories[memory_id]
    
    return json.dumps({
        "status": "success",
        "message": f"M√©moire {memory_id} supprim√©e du cerveau collectif (m√©moire)"
    })

def list_memories(user_id: str = None, team_id: str = None) -> str:
    """Lister toutes les m√©moires du cerveau collectif avec logique unifi√©e"""
    
    # Utiliser la logique unifi√©e pour obtenir le contexte
    if not (user_id and team_id):
        user_token = extract_token_from_headers() or "test-token"
        context = get_unified_team_context(
            user_token=user_token,
            user_id=user_id,
            team_id=team_id
        )
        user_id = context["user_id"]
        team_id = context["team_id"]
        user_name = context["user_name"]
    else:
        user_name = user_id.replace("user_", "").replace("_", " ").title()
    
    print(f"üìã Liste m√©moires - User: {user_id}, Team: {team_id}")
    
    storage = get_storage()
    if storage:
        try:
            all_memories = storage.list_memories(team_id)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur Qdrant, fallback vers m√©moire: {e}")
            all_memories = []
    else:
        all_memories = []
    
    # Si pas de r√©sultats de Qdrant, utiliser le stockage en m√©moire
    if not all_memories:
        if not memories:
            return json.dumps({
                "status": "success",
                "message": "Aucune m√©moire dans le cerveau collectif",
                "total": 0,
                "memories": [],
                "user": user_name,
                "team": team_id
            })
        
        for memory_id, memory in memories.items():
            if memory.team_id == team_id:  # Isolation par √©quipe
                all_memories.append({
                    "memory_id": memory_id,
                    "content": memory.content,
                    "tags": memory.tags,
                    "timestamp": memory.timestamp,
                    "user_id": memory.user_id,
                    "category": memory.category,
                    "confidence": memory.confidence
                })
    
    return json.dumps({
        "status": "success",
        "total": len(all_memories),
        "memories": all_memories,
        "user": user_name,
        "team": team_id
    })

def get_team_insights() -> str:
    """Obtenir des insights sur l'activit√© de l'√©quipe avec authentification"""
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    
    # MODE TEST: Si aucun token trouv√©, utiliser le token de test
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    if not user_token:
        return json.dumps({
            "status": "error",
            "message": "Token utilisateur requis. Veuillez configurer l'authentification Bearer dans Le Chat."
        })
    
    # V√©rifier le token utilisateur
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({
            "status": "error",
            "message": f"Token utilisateur invalide: {user_token[:10]}..."
        })
    
    team_id = user_info["team_id"]
    user_name = user_info["user_name"]
    
    # R√©cup√©rer toutes les m√©moires de l'√©quipe
    all_memories = []
    storage = get_storage()
    
    if storage:
        try:
            all_memories = storage.list_memories(team_id)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur Qdrant, fallback vers m√©moire: {e}")
    
    # Fallback vers m√©moire locale
    if not all_memories:
        for memory_id, memory in memories.items():
            if memory.team_id == team_id:
                all_memories.append({
                    "memory_id": memory_id,
                    "content": memory.content,
                    "category": getattr(memory, 'category', 'general'),
                    "tags": memory.tags,
                    "timestamp": memory.timestamp,
                    "user_id": getattr(memory, 'user_id', 'unknown'),
                    "confidence": getattr(memory, 'confidence', 0.5)
                })
    
    # Analyser les patterns
    categories = {}
    contributors = {}
    tags_count = {}
    high_confidence = 0
    
    for memory in all_memories:
        # Compter les cat√©gories
        category = memory.get('category', 'general')
        categories[category] = categories.get(category, 0) + 1
        
        # Compter les contributeurs
        user = memory.get('user_id', 'unknown')
        contributors[user] = contributors.get(user, 0) + 1
        
        # Compter les tags
        for tag in memory.get('tags', []):
            tags_count[tag] = tags_count.get(tag, 0) + 1
        
        # Compter les m√©moires importantes
        if memory.get('confidence', 0) > 0.7:
            high_confidence += 1
    
    # Top 5 de chaque cat√©gorie
    top_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)[:5]
    top_contributors = sorted(contributors.items(), key=lambda x: x[1], reverse=True)[:5]
    top_tags = sorted(tags_count.items(), key=lambda x: x[1], reverse=True)[:5]
    
    insights = {
        "team_id": team_id,
        "total_memories": len(all_memories),
        "high_confidence_memories": high_confidence,
        "top_categories": [{"category": cat, "count": count} for cat, count in top_categories],
        "top_contributors": [{"user": user, "contributions": count} for user, count in top_contributors],
        "trending_tags": [{"tag": tag, "count": count} for tag, count in top_tags],
        "knowledge_health": {
            "coverage": len(categories),
            "activity_level": "high" if len(all_memories) > 20 else "medium" if len(all_memories) > 5 else "low",
            "collaboration_score": len(contributors) / max(len(all_memories), 1)
        }
    }
    
    return json.dumps({
        "status": "success",
        "insights": insights,
        "user": user_name,
        "team": team_id
    })

def record_conversation_message(
    chat_id: str,
    role: str,
    content: str,
    title: str = ""
) -> str:
    """Enregistrer un message dans une conversation avec authentification"""
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    
    # MODE TEST: Si aucun token trouv√©, utiliser le token de test
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    if not user_token:
        return json.dumps({
            "status": "error",
            "message": "Token utilisateur requis. Veuillez configurer l'authentification Bearer dans Le Chat."
        })
    
    # V√©rifier le token utilisateur
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({
            "status": "error",
            "message": f"Token utilisateur invalide: {user_token[:10]}..."
        })
    
    user_id = user_info["user_id"]
    team_id = user_info["team_id"]
    user_name = user_info["user_name"]
    
    # V√©rifier que le r√¥le est valide
    if role not in ["user", "assistant"]:
        return json.dumps({
            "status": "error",
            "message": "Le r√¥le doit √™tre 'user' ou 'assistant'"
        })
    
    # Cr√©er ou r√©cup√©rer la conversation
    if chat_id not in conversations:
        conversations[chat_id] = Conversation(
            chat_id=chat_id,
            team_id=team_id,
            user_id=user_id,
            title=title or f"Conversation {chat_id[:8]}",
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat()
        )
    
    conversation = conversations[chat_id]
    
    # V√©rifier que l'utilisateur appartient √† la m√™me √©quipe
    if conversation.team_id != team_id:
        return json.dumps({
            "status": "error",
            "message": "Acc√®s non autoris√© √† cette conversation"
        })
    
    # Cr√©er le message
    message = ConversationMessage(
        role=role,
        content=content,
        timestamp=datetime.now().isoformat(),
        user_id=user_id
    )
    
    # Ajouter le message √† la conversation
    conversation.messages.append(message)
    conversation.updated_at = datetime.now().isoformat()
    
    # G√©n√©rer un r√©sum√© si c'est le 5√®me message ou plus
    if len(conversation.messages) >= 5 and not conversation.summary:
        conversation.summary = generate_conversation_summary(conversation)
    
    return json.dumps({
        "status": "success",
        "message": "Message enregistr√© avec succ√®s",
        "chat_id": chat_id,
        "message_count": len(conversation.messages),
        "has_summary": bool(conversation.summary),
        "user": user_name,
        "team": team_id
    })

def get_conversation_summary(chat_id: str) -> str:
    """Obtenir le r√©sum√© d'une conversation avec authentification"""
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    
    # MODE TEST: Si aucun token trouv√©, utiliser le token de test
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    if not user_token:
        return json.dumps({
            "status": "error",
            "message": "Token utilisateur requis. Veuillez configurer l'authentification Bearer dans Le Chat."
        })
    
    # V√©rifier le token utilisateur
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({
            "status": "error",
            "message": f"Token utilisateur invalide: {user_token[:10]}..."
        })
    
    team_id = user_info["team_id"]
    user_name = user_info["user_name"]
    
    # V√©rifier que la conversation existe
    if chat_id not in conversations:
        return json.dumps({
            "status": "error",
            "message": "Conversation non trouv√©e"
        })
    
    conversation = conversations[chat_id]
    
    # V√©rifier que l'utilisateur appartient √† la m√™me √©quipe
    if conversation.team_id != team_id:
        return json.dumps({
            "status": "error",
            "message": "Acc√®s non autoris√© √† cette conversation"
        })
    
    # G√©n√©rer un r√©sum√© si n√©cessaire
    if not conversation.summary and len(conversation.messages) > 0:
        conversation.summary = generate_conversation_summary(conversation)
    
    return json.dumps({
        "status": "success",
        "chat_id": chat_id,
        "title": conversation.title,
        "message_count": len(conversation.messages),
        "summary": conversation.summary or "Aucun r√©sum√© disponible",
        "created_at": conversation.created_at,
        "updated_at": conversation.updated_at,
        "user": user_name,
        "team": team_id
    })

def list_team_conversations(limit: int = 10) -> str:
    """Lister les conversations de l'√©quipe avec authentification"""
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    
    # MODE TEST: Si aucun token trouv√©, utiliser le token de test
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    if not user_token:
        return json.dumps({
            "status": "error",
            "message": "Token utilisateur requis. Veuillez configurer l'authentification Bearer dans Le Chat."
        })
    
    # V√©rifier le token utilisateur
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({
            "status": "error",
            "message": f"Token utilisateur invalide: {user_token[:10]}..."
        })
    
    team_id = user_info["team_id"]
    user_name = user_info["user_name"]
    
    # Filtrer les conversations de l'√©quipe
    team_conversations = []
    for conv in conversations.values():
        if conv.team_id == team_id:
            team_conversations.append({
                "chat_id": conv.chat_id,
                "title": conv.title,
                "message_count": len(conv.messages),
                "has_summary": bool(conv.summary),
                "created_at": conv.created_at,
                "updated_at": conv.updated_at,
                "last_message": conv.messages[-1].content[:100] + "..." if conv.messages else ""
            })
    
    # Trier par date de mise √† jour (plus r√©cent en premier)
    team_conversations.sort(key=lambda x: x["updated_at"], reverse=True)
    
    return json.dumps({
        "status": "success",
        "conversations": team_conversations[:limit],
        "total": len(team_conversations),
        "user": user_name,
        "team": team_id
    })

def generate_conversation_insights(chat_id: str) -> str:
    """G√©n√©rer des insights d√©taill√©s sur une conversation avec Mistral"""
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    
    # MODE TEST: Si aucun token trouv√©, utiliser le token de test
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    if not user_token:
        return json.dumps({
            "status": "error",
            "message": "Token utilisateur requis. Veuillez configurer l'authentification Bearer dans Le Chat."
        })
    
    # V√©rifier le token utilisateur
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({
            "status": "error",
            "message": f"Token utilisateur invalide: {user_token[:10]}..."
        })
    
    team_id = user_info["team_id"]
    user_name = user_info["user_name"]
    
    # V√©rifier que la conversation existe
    if chat_id not in conversations:
        return json.dumps({
            "status": "error",
            "message": "Conversation non trouv√©e"
        })
    
    conversation = conversations[chat_id]
    
    # V√©rifier que l'utilisateur appartient √† la m√™me √©quipe
    if conversation.team_id != team_id:
        return json.dumps({
            "status": "error",
            "message": "Acc√®s non autoris√© √† cette conversation"
        })
    
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        return json.dumps({
            "status": "error",
            "message": "Mistral non configur√© pour les insights"
        })
    
    try:
        ensure_mistral_import()
        if not Mistral:
            return json.dumps({
                "status": "error",
                "message": "Mistral non disponible"
            })
        
        # Pr√©parer le contexte de la conversation
        conversation_text = ""
        for msg in conversation.messages:
            role = "Utilisateur" if msg.role == "user" else "Assistant"
            conversation_text += f"{role}: {msg.content}\n"
        
        # Prompt pour les insights avec timestamp
        current_time = datetime.now()
        prompt = f"""Analysez cette conversation et fournissez des insights structur√©s en JSON.
        
        Date et heure actuelle: {current_time.strftime("%Y-%m-%d %H:%M:%S")}
        
        Conversation:
        {conversation_text}
        
        Retournez un JSON avec:
        - "key_topics": liste des sujets principaux
        - "decisions": liste des d√©cisions prises
        - "action_items": liste des actions √† faire
        - "sentiment": sentiment g√©n√©ral (positif/neutre/n√©gatif)
        - "complexity": niveau de complexit√© (faible/moyen/√©lev√©)
        - "recommendations": recommandations pour la suite
        - "timeline": r√©f√©rences temporelles mentionn√©es dans la conversation
        
        Format JSON uniquement:"""
        
        # Appel √† l'API Mistral
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.2
        )
        
        insights_text = response.choices[0].message.content.strip()
        
        # Essayer de parser le JSON
        try:
            insights = json.loads(insights_text)
        except:
            insights = {"raw_insights": insights_text}
        
        return json.dumps({
            "status": "success",
            "chat_id": chat_id,
            "insights": insights,
            "message_count": len(conversation.messages),
            "user": user_name,
            "team": team_id
        })
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration insights: {e}")
        return json.dumps({
            "status": "error",
            "message": f"Erreur lors de la g√©n√©ration des insights: {e}"
        })

# --- Nouvelles fonctionnalit√©s de Chat Interactif ---

# Dictionnaire pour stocker les conversations en cours
active_chats: Dict[str, List[Dict[str, str]]] = {}

def save_conversation_to_file(chat_id: str, conversation_history: List) -> str:
    """Sauvegarder une conversation en continu dans un fichier .txt (uniquement messages visibles par l'utilisateur)"""
    try:
        # Cr√©er le r√©pertoire transcripts s'il n'existe pas
        transcript_dir = "transcripts"
        os.makedirs(transcript_dir, exist_ok=True)
        
        # Chemin du fichier de transcription
        transcript_path = os.path.join(transcript_dir, f"{chat_id}.txt")
        
        # Convertir la conversation en format texte (uniquement messages visibles)
        conversation_text = ""
        for msg in conversation_history:
            # G√©rer les diff√©rents types de messages (dict ou objet Mistral)
            if isinstance(msg, dict):
                role = msg.get('role', '')
                content = msg.get('content', '')
            else:
                # Objet Mistral (AssistantMessage, etc.)
                role = getattr(msg, 'role', '')
                content = getattr(msg, 'content', '')
            
            # Filtrer uniquement les messages utilisateur et assistant (pas system, tool, etc.)
            if role in ['user', 'assistant']:
                # Pour les messages assistant, v√©rifier qu'ils ne contiennent pas d'instructions syst√®me
                if role == 'assistant':
                    # Ignorer les messages qui contiennent des instructions syst√®me ou des m√©moires
                    if any(keyword in content.lower() for keyword in [
                        'tu es un assistant', 'instructions importantes', 'outils disponibles',
                        'r√®gles sp√©ciales', 'date et heure actuelle', 'portfolio utilisateur',
                        'm√©moire collective', 'conversations pass√©es', 'attendez confirmation',
                        '‚ö†Ô∏è attention', 'cat√©gorie:', 'date:', 'score:', '--- m√©moire',
                        'üìö m√©moires pertinentes', 'üí¨ conversations pertinentes', 'üîç recherche',
                        'üõ†Ô∏è appels d\'outils', 'üîÑ it√©ration', '‚úÖ r√©ponse finale'
                    ]):
                        continue
                
                # Pour les messages utilisateur, v√©rifier qu'ils ne contiennent pas d'enrichissement
                if role == 'user':
                    # Ignorer les messages qui contiennent des m√©moires ou conversations enrichies
                    if any(keyword in content for keyword in [
                        'üìö M√âMOIRES PERTINENTES', 'üí¨ CONVERSATIONS PERTINENTES',
                        '--- M√©moire', '--- Conversation', 'Score:', 'Cat√©gorie:',
                        '‚ö†Ô∏è ATTENTION:', 'Utilise ces m√©moires', 'Utilise ces conversations'
                    ]):
                        continue
                
                role_display = "Utilisateur" if role == 'user' else "Assistant"
                conversation_text += f"{role_display}: {content}\n"
        
        # √âcraser le fichier avec la conversation compl√®te
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(conversation_text)
        
        print(f"üíæ Conversation sauvegard√©e en continu: {transcript_path}")
        return transcript_path
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde en continu: {e}")
        return ""

def get_conversation_transcript(chat_id: str) -> str:
    """R√©cup√©rer le contenu d'un fichier de transcription de conversation"""
    try:
        transcript_path = os.path.join("transcripts", f"{chat_id}.txt")
        
        if not os.path.exists(transcript_path):
            return f"‚ùå Transcription non trouv√©e pour le chat_id: {chat_id}"
        
        with open(transcript_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        return content
        
    except Exception as e:
        return f"‚ùå Erreur lors de la lecture de la transcription: {e}"

def list_available_transcripts() -> str:
    """Lister tous les fichiers de transcription disponibles"""
    try:
        transcript_dir = "transcripts"
        if not os.path.exists(transcript_dir):
            return "‚ùå Aucun dossier transcripts trouv√©"
        
        files = [f for f in os.listdir(transcript_dir) if f.endswith('.txt')]
        
        if not files:
            return "‚ùå Aucun fichier de transcription trouv√©"
        
        result = f"üìÅ {len(files)} fichiers de transcription disponibles:\n\n"
        for i, file in enumerate(sorted(files), 1):
            chat_id = file.replace('.txt', '')
            file_path = os.path.join(transcript_dir, file)
            file_size = os.path.getsize(file_path)
            result += f"{i}. {chat_id} ({file_size} bytes)\n"
        
        return result
        
    except Exception as e:
        return f"‚ùå Erreur lors de la liste des transcriptions: {e}"

# Syst√®me de limitation des rappels par conversation
reminder_notifications = {}  # {chat_id: {"last_reminder_time": timestamp, "reminder_count": int}}

def chat_with_mistral(
    chat_id: str,
    user_message: str,
    user_id: str = None,
    team_id: str = None
) -> str:
    """
    G√©rer une conversation interactive avec Mistral Large,
    avec logique de team_id unifi√©e
    """
    
    # R√©cup√©rer le token depuis les headers
    user_token = extract_token_from_headers()
    if not user_token:
        user_token = "test-token"  # Fallback pour les tests
    
    # ‚úÖ UTILISER LA LOGIQUE UNIFI√âE
    context = get_unified_team_context(
        user_token=user_token,
        user_id=user_id,
        team_id=team_id
    )
    
    user_id = context["user_id"]
    team_id = context["team_id"]
    user_name = context["user_name"]
    
    print(f"üîÑ Context unifi√©: {context}")

    # G√©n√©rer un chat_id unique si non fourni
    if not chat_id:
        chat_id = hashlib.md5(f"{user_id}-{datetime.now().isoformat()}".encode()).hexdigest()
        print(f"üÜï Nouvelle conversation initi√©e avec chat_id: {chat_id}")

    # Mode test : simuler Mistral si non configur√©
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        # Mode simulation pour les tests
        assistant_response = f"Mode test : J'ai re√ßu votre message '{user_message}'. Dans un environnement de production avec Mistral configur√©, je pourrais utiliser tous les outils de m√©moire collective pour vous aider."
        
        # Ajouter la r√©ponse simul√©e √† l'historique
        if chat_id not in active_chats:
            active_chats[chat_id] = []
        active_chats[chat_id].append({"role": "assistant", "content": assistant_response})
        
        return json.dumps({
            "status": "success", 
            "response": assistant_response,
            "mode": "test_simulation"
        })
    
    ensure_mistral_import()
    if not Mistral:
        return json.dumps({"status": "error", "message": "Mistral non disponible."})

    # V√©rifier la disponibilit√© du stockage
    storage = get_storage()
    storage_available = storage is not None
    
    # Processus de r√©flexion pr√©alable avec portfolio
    print("üß† D√©but du processus de r√©flexion...")
    add_ai_log("thinking", "D√©but du processus de r√©flexion...")
    thinking_result = reflective_thinking_process(user_message, user_id, team_id)
    print(f"üß† Processus de r√©flexion: {thinking_result}")
    add_ai_log("thinking", f"Processus de r√©flexion termin√©: {thinking_result.get('patterns_detected', [])}")
    
    # Si une r√®gle du portfolio est d√©clench√©e, r√©pondre imm√©diatement
    if thinking_result.get("rule_triggered") and thinking_result.get("response"):
        print(f"‚ö° R√®gle portfolio d√©clench√©e: {thinking_result['response']}")
        add_ai_log("portfolio", f"R√®gle portfolio d√©clench√©e: {thinking_result['response']}")
        return json.dumps({
            "status": "success",
            "response": thinking_result["response"],
            "chat_id": chat_id,
            "mode": "portfolio_rule_triggered",
            "source": thinking_result.get("source", "portfolio")
        })
    
    # Analyser la conversation pour d√©tecter des rappels potentiels
    if thinking_result.get("time_references") or thinking_result.get("patterns_detected"):
        print("üîî Analyse de la conversation pour d√©tecter des rappels...")
        conversation_text = f"Utilisateur: {user_message}"
        detected_reminders = analyze_conversation_for_reminders(conversation_text, user_id, team_id)
        
        if detected_reminders:
            print(f"üîî {len(detected_reminders)} rappels d√©tect√©s automatiquement")
            # Ajouter les rappels d√©tect√©s
            for reminder in detected_reminders:
                add_reminder(
                    title=reminder.get("title", "Rappel d√©tect√©"),
                    description=reminder.get("description", ""),
                    due_date=reminder.get("due_date", ""),
                    priority=reminder.get("priority", "medium"),
                    reminder_type=reminder.get("type", "general")
                )
    
    # Mise √† jour automatique du portfolio bas√©e sur le message
    print("üìã Mise √† jour automatique du portfolio...")
    add_ai_log("portfolio", "Mise √† jour automatique du portfolio...")
    auto_update_portfolio(user_message, user_id, team_id)
    
    # V√©rification automatique des rappels √† venir
    print("üîî V√©rification des rappels √† venir...")
    add_ai_log("reminder", "V√©rification des rappels √† venir...")
    upcoming_reminders = check_upcoming_reminders(team_id, hours_ahead=24)
    reminders_context = ""
    
    # D√©tecter et supprimer les rappels en double
    if upcoming_reminders:
        original_count = len(upcoming_reminders)
        upcoming_reminders = detect_duplicate_reminders(upcoming_reminders)
        if len(upcoming_reminders) < original_count:
            print(f"üîÑ {original_count - len(upcoming_reminders)} rappels en double supprim√©s")
    
    # Utiliser la logique intelligente pour d√©cider d'afficher les rappels
    if upcoming_reminders and should_show_reminders(chat_id, upcoming_reminders):
        print(f"üîî {len(upcoming_reminders)} rappels √† venir d√©tect√©s (affichage autoris√©)")
        add_ai_log("reminder", f"{len(upcoming_reminders)} rappels √† venir d√©tect√©s")
        
        # Pr√©parer le contexte des rappels √† venir
        reminders_context = "\n\nüîî RAPPELS √Ä VENIR :\n"
        for reminder in upcoming_reminders[:3]:  # Limiter √† 3 rappels
            minutes_until = reminder.get("minutes_until", 0)
            hours_until = reminder.get("hours_until", 0)
            
            # Formatage du temps plus pr√©cis
            if minutes_until < 60:
                time_str = f"dans {int(minutes_until)} minutes"
            elif hours_until < 24:
                time_str = f"dans {hours_until:.1f} heures"
            else:
                days = hours_until / 24
                time_str = f"dans {days:.1f} jours"
            
            reminders_context += f"‚Ä¢ {reminder.get('title', 'Rappel')} - {time_str}\n"
    elif upcoming_reminders:
        print(f"üîî {len(upcoming_reminders)} rappels √† venir d√©tect√©s (affichage limit√©)")
        add_ai_log("reminder", f"{len(upcoming_reminders)} rappels √† venir d√©tect√©s (non affich√©s)")
    
    # Historique de la conversation
    if chat_id not in active_chats:
        current_time = datetime.now()
        system_message = f"""Tu es un assistant IA expert avec acc√®s √† une m√©moire collective, aux conversations pass√©es, et au PORTFOLIO UTILISATEUR DYNAMIQUE.

DATE ET HEURE ACTUELLE: {current_time.strftime("%Y-%m-%d %H:%M:%S")} (Fuseau horaire: {current_time.strftime("%Z")})

INSTRUCTIONS IMPORTANTES :
- R√©ponds de mani√®re naturelle et conversationnelle
- N'affiche JAMAIS les √©tapes de r√©flexion internes dans tes r√©ponses
- Utilise les outils disponibles pour acc√©der aux informations
- V√©rifie automatiquement le portfolio utilisateur et les rappels √† venir
- Mets √† jour le portfolio avec les nouvelles informations importantes
- Sois concis et utile
- ‚ö†Ô∏è IMPORTANT: Pour les rappels, calcule TOUJOURS l'heure exacte en ajoutant les minutes √† l'heure actuelle. Ne devine jamais l'heure !
- ‚ö†Ô∏è CRITIQUE: Si des m√©moires ou conversations sont fournies automatiquement, tu DOIS les utiliser dans ta r√©ponse. Ne dis jamais "je ne sais pas" si des informations pertinentes sont disponibles !

OUTILS DISPONIBLES :
- search_memories : Rechercher dans la m√©moire collective
- search_past_conversations : Rechercher dans les conversations archiv√©es
- get_user_portfolio_summary : Consulter le portfolio utilisateur
- update_user_portfolio : Mettre √† jour le portfolio
- check_upcoming_reminders : V√©rifier les rappels √† venir
- add_reminder : Ajouter un rappel

R√àGLES SP√âCIALES :
- V√©rifie toujours les rappels √† venir et informe l'utilisateur
- Enrichis automatiquement le portfolio avec les informations importantes
- Sois proactif dans la gestion des rappels et √©v√©nements

INSTRUCTIONS DE RECHERCHE OBLIGATOIRES :
- ‚ö†Ô∏è TU DOIS TOUJOURS rechercher dans les m√©moires et conversations avant de r√©pondre
- Si tu n'as pas d'informations sur un sujet, utilise search_memories et search_past_conversations
- Recherche toujours dans les m√©moires avant de dire "je ne sais pas"
- Utilise des termes de recherche pertinents pour trouver des informations
- ‚ö†Ô∏è CRITIQUE: Si des m√©moires sont fournies automatiquement, tu DOIS les utiliser en priorit√© !
- üö´ NE FAIS PAS de recherches suppl√©mentaires si des m√©moires pertinentes sont d√©j√† fournies !
- Ne r√©ponds JAMAIS "je ne sais pas" si des m√©moires pertinentes sont disponibles !

R√©ponds de mani√®re naturelle et utile !{reminders_context}"""
        
        if not storage_available:
            system_message += "\n\nNote: Le service de stockage vectoriel n'est pas disponible pour le moment, mais tu peux toujours r√©pondre aux questions g√©n√©rales."
        
        active_chats[chat_id] = [{"role": "system", "content": system_message}]
    
    # Int√©grer les m√©moires trouv√©es automatiquement dans le message utilisateur
    enhanced_user_message = user_message
    
    if storage_available:
        try:
            # Recherche dans les m√©moires avec des termes plus sp√©cifiques
            search_terms = [user_message]
            
            # Extraire des mots-cl√©s sp√©cifiques pour am√©liorer la recherche
            if "baptiste" in user_message.lower():
                search_terms.extend(["baptiste code", "baptiste flutter", "baptiste react", "baptiste typescript"])
            if "code" in user_message.lower():
                search_terms.extend(["code", "programming", "d√©veloppement"])
            if "travaille" in user_message.lower() or "travail" in user_message.lower():
                search_terms.extend(["travail", "projet", "work"])
            
            # Recherche avec le terme original
            memory_results = storage.search_memories(user_message, team_id, limit=5)
            
            # Recherche avec des termes sp√©cifiques pour Baptiste
            if "baptiste" in user_message.lower():
                for term in ["baptiste code", "baptiste flutter", "baptiste react"]:
                    additional_results = storage.search_memories(term, team_id, limit=3)
                    # Fusionner les r√©sultats en √©vitant les doublons
                    for result in additional_results:
                        if not any(r.get('memory_id') == result.get('memory_id') for r in memory_results):
                            memory_results.append(result)
                
                # Trier par priorit√© : m√©moires de travail d'abord, puis par score
                def priority_score(result):
                    category = result.get('category', '')
                    base_score = result.get('similarity_score', 0)
                    content = result.get('content', '').lower()
                    
                    # Prioriser les m√©moires de travail
                    if category in ['work', 'project', 'code']:
                        return base_score + 0.2  # Bonus pour les m√©moires de travail
                    # Prioriser les conversations qui mentionnent des technologies sp√©cifiques
                    elif category == 'conversation_summary' and any(tech in content for tech in ['flutter', 'react', 'typescript', 'javascript', 'python', 'java']):
                        return base_score + 0.3  # Bonus important pour les conversations techniques
                    elif category == 'conversation_summary':
                        return base_score - 0.1  # Malus pour les conversations g√©n√©rales
                    else:
                        return base_score
                
                memory_results.sort(key=priority_score, reverse=True)
                memory_results = memory_results[:5]  # Garder seulement les 5 meilleurs
            if memory_results:
                print(f"‚úÖ {len(memory_results)} m√©moires trouv√©es automatiquement")
                add_ai_log("search", f"{len(memory_results)} m√©moires trouv√©es automatiquement")
                
                # Int√©grer les m√©moires dans le message utilisateur
                enhanced_user_message += "\\n\\nüìö M√âMOIRES PERTINENTES TROUV√âES:\\n"
                for i, mem in enumerate(memory_results, 1):
                    enhanced_user_message += f"\\n--- M√©moire {i} (Score: {mem.get('similarity_score', 0):.3f}) ---\\n"
                    enhanced_user_message += f"Contenu: {mem.get('content', 'N/A')}\\n"
                    enhanced_user_message += f"Cat√©gorie: {mem.get('category', 'N/A')}\\n"
                    enhanced_user_message += f"Date: {mem.get('timestamp', 'N/A')}\\n"
                
                enhanced_user_message += "\\n‚ö†Ô∏è ATTENTION: Utilise ces m√©moires dans ta r√©ponse !\\n"
            
            # Recherche dans les conversations archiv√©es
            conversation_results = storage.search_conversation_summaries(user_message, team_id, limit=2)
            if conversation_results:
                print(f"‚úÖ {len(conversation_results)} conversations trouv√©es automatiquement")
                add_ai_log("search", f"{len(conversation_results)} conversations trouv√©es automatiquement")
                
                # Int√©grer les conversations dans le message utilisateur
                enhanced_user_message += "\\n\\nüí¨ CONVERSATIONS PERTINENTES TROUV√âES:\\n"
                for i, conv in enumerate(conversation_results, 1):
                    enhanced_user_message += f"\\n--- Conversation {i} (Score: {conv.get('score', 0):.3f}) ---\\n"
                    enhanced_user_message += f"R√©sum√©: {conv.get('summary', 'N/A')}\\n"
                    enhanced_user_message += f"Date: {conv.get('timestamp', 'N/A')}\\n"
                
                enhanced_user_message += "\\n‚ö†Ô∏è ATTENTION: Utilise ces conversations dans ta r√©ponse !\\n"
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la recherche automatique: {e}")
            add_ai_log("search", f"Erreur recherche automatique: {e}")
    
    # Ajouter le message utilisateur enrichi √† la conversation active (pour l'IA)
    active_chats[chat_id].append({"role": "user", "content": enhanced_user_message})

    # D√©tecter les IDs de conversation dans le message de l'utilisateur
    detected_ids = detect_conversation_ids(user_message)
    if detected_ids:
        print(f"üîç IDs de conversation d√©tect√©s: {detected_ids}")
        # Ajouter une instruction sp√©ciale pour forcer l'utilisation de ces IDs
        special_instruction = f"\\n\\nATTENTION: L'utilisateur a mentionn√© les IDs de conversation suivants: {', '.join(detected_ids)}. Tu DOIS utiliser `get_full_transcript` avec ces IDs EXACTS en priorit√© absolue. C'est la premi√®re chose √† faire."
        active_chats[chat_id].append({"role": "system", "content": special_instruction})

    try:
        # G√©n√©rer dynamiquement le sch√©ma des outils √† partir de available_tools
        tools_for_mistral = []
        
        # Sch√©mas sp√©cifiques pour les outils les plus importants
        tool_schemas = {
            "search_past_conversations": {
                "type": "function",
                "function": {
                    "name": "search_past_conversations",
                    "description": "Rechercher dans les r√©sum√©s des conversations archiv√©es",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Terme de recherche pour trouver des conversations pertinentes"
                            },
                            "limit": {
                                "type": "integer",
                                "description": "Nombre maximum de r√©sultats √† retourner",
                                "default": 3
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            "search_memories": {
                "type": "function",
                "function": {
                    "name": "search_memories",
                    "description": "Rechercher dans la m√©moire collective de l'√©quipe",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Terme de recherche dans les m√©moires"
                            },
                            "limit": {
                                "type": "integer",
                                "description": "Nombre maximum de r√©sultats √† retourner",
                                "default": 5
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            "add_memory": {
                "type": "function",
                "function": {
                    "name": "add_memory",
                    "description": "Ajouter une m√©moire au cerveau collectif de l'√©quipe",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "content": {
                                "type": "string",
                                "description": "Contenu de la m√©moire √† ajouter"
                            },
                            "tags": {
                                "type": "string",
                                "description": "Tags s√©par√©s par des virgules",
                                "default": ""
                            },
                            "category": {
                                "type": "string",
                                "description": "Cat√©gorie de la m√©moire",
                                "default": "general"
                            },
                            "visibility": {
                                "type": "string",
                                "description": "Visibilit√© de la m√©moire",
                                "default": "team"
                            }
                        },
                        "required": ["content"]
                    }
                }
            },
            "get_full_transcript": {
                "type": "function",
                "function": {
                    "name": "get_full_transcript",
                    "description": "R√©cup√©rer la transcription compl√®te d'une conversation",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "chat_id": {
                                "type": "string",
                                "description": "ID de la conversation √† r√©cup√©rer"
                            }
                        },
                        "required": ["chat_id"]
                    }
                }
            },
            "multi_query_search": {
                "type": "function",
                "function": {
                    "name": "multi_query_search",
                    "description": "Effectuer plusieurs requ√™tes de recherche avec diff√©rentes approches pour maximiser les r√©sultats",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Terme de recherche principal"
                            },
                            "max_queries": {
                                "type": "integer",
                                "description": "Nombre maximum de variantes de recherche √† effectuer",
                                "default": 5
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            "search_in_transcript": {
                "type": "function",
                "function": {
                    "name": "search_in_transcript",
                    "description": "Rechercher un terme sp√©cifique dans la transcription compl√®te d'une conversation",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "chat_id": {
                                "type": "string",
                                "description": "ID de la conversation √† rechercher"
                            },
                            "search_term": {
                                "type": "string",
                                "description": "Terme √† rechercher dans la transcription"
                            }
                        },
                        "required": ["chat_id", "search_term"]
                    }
                }
            },
            "search_archived_conversations": {
                "type": "function",
                "function": {
                    "name": "search_archived_conversations",
                    "description": "Recherche dans toutes les conversations archiv√©es avec une approche exhaustive",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Terme de recherche pour trouver des conversations pertinentes"
                            },
                            "limit": {
                                "type": "integer",
                                "description": "Nombre maximum de r√©sultats √† retourner",
                                "default": 10
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            "expand_search_keywords": {
                "type": "function",
                "function": {
                    "name": "expand_search_keywords",
                    "description": "G√©n√®re des mots-cl√©s de recherche alternatifs et des synonymes pour une requ√™te",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Requ√™te de recherche originale"
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            "add_user_rule": {
                "type": "function",
                "function": {
                    "name": "add_user_rule",
                    "description": "Ajouter une r√®gle utilisateur personnalis√©e qui sera v√©rifi√©e √† chaque message",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "pattern": {
                                "type": "string",
                                "description": "Le pattern √† d√©tecter (ex: 'ends_with_quoi', 'contains_quoi', 'exact_quoi', ou une regex)"
                            },
                            "response": {
                                "type": "string",
                                "description": "La r√©ponse √† donner quand le pattern est d√©tect√©"
                            },
                            "rule_type": {
                                "type": "string",
                                "description": "Type de r√®gle: 'pattern_match', 'keyword_match', 'exact_match'",
                                "default": "pattern_match"
                            },
                            "description": {
                                "type": "string",
                                "description": "Description de la r√®gle",
                                "default": ""
                            }
                        },
                        "required": ["pattern", "response"]
                    }
                }
            },
            "update_user_portfolio": {
                "type": "function",
                "function": {
                    "name": "update_user_portfolio",
                    "description": "Mettre √† jour le portfolio utilisateur avec de nouvelles informations",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "user_id": {
                                "type": "string",
                                "description": "ID de l'utilisateur"
                            },
                            "updates": {
                                "type": "object",
                                "description": "Objet contenant les mises √† jour du portfolio (profile, rules, context, events, reminders, learning)"
                            }
                        },
                        "required": ["user_id", "updates"]
                    }
                }
            },
            "get_user_portfolio_summary": {
                "type": "function",
                "function": {
                    "name": "get_user_portfolio_summary",
                    "description": "Obtenir un r√©sum√© du portfolio utilisateur",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "user_id": {
                                "type": "string",
                                "description": "ID de l'utilisateur"
                            }
                        },
                        "required": ["user_id"]
                    }
                }
            },
            "add_reminder": {
                "type": "function",
                "function": {
                    "name": "add_reminder",
                    "description": "Ajouter un rappel intelligent pour l'utilisateur",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "Titre du rappel"
                            },
                            "description": {
                                "type": "string",
                                "description": "Description d√©taill√©e du rappel"
                            },
                            "due_date": {
                                "type": "string",
                                "description": "Date/heure du rappel (format ISO ou description naturelle)"
                            },
                            "priority": {
                                "type": "string",
                                "description": "Priorit√©: low, medium, high",
                                "default": "medium"
                            },
                            "reminder_type": {
                                "type": "string",
                                "description": "Type de rappel: meeting, deadline, personal, work, other",
                                "default": "general"
                            },
                            "user_id": {
                                "type": "string",
                                "description": "ID de l'utilisateur (inject√© automatiquement)"
                            },
                            "team_id": {
                                "type": "string",
                                "description": "ID de l'√©quipe (inject√© automatiquement)"
                            }
                        },
                        "required": ["title", "description", "due_date"]
                    }
                }
            },
            "check_upcoming_reminders": {
                "type": "function",
                "function": {
                    "name": "check_upcoming_reminders",
                    "description": "V√©rifier les rappels √† venir pour l'utilisateur",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "team_id": {
                                "type": "string",
                                "description": "ID de l'√©quipe"
                            },
                            "hours_ahead": {
                                "type": "integer",
                                "description": "Nombre d'heures √† l'avance pour v√©rifier les rappels",
                                "default": 24
                            }
                        },
                        "required": ["team_id"]
                    }
                }
            },
            "list_reminders": {
                "type": "function",
                "function": {
                    "name": "list_reminders",
                    "description": "Lister tous les rappels de l'utilisateur",
                    "parameters": {
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                }
            },
            "delete_reminder": {
                "type": "function",
                "function": {
                    "name": "delete_reminder",
                    "description": "Supprimer un rappel sp√©cifique par son ID",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "reminder_id": {
                                "type": "string",
                                "description": "ID du rappel √† supprimer"
                            },
                            "user_id": {
                                "type": "string",
                                "description": "ID de l'utilisateur (inject√© automatiquement)"
                            },
                            "team_id": {
                                "type": "string",
                                "description": "ID de l'√©quipe (inject√© automatiquement)"
                            }
                        },
                        "required": ["reminder_id"]
                    }
                }
            },
            "delete_all_reminders": {
                "type": "function",
                "function": {
                    "name": "delete_all_reminders",
                    "description": "Supprimer tous les rappels de l'utilisateur",
                    "parameters": {
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                }
            }
        }
        
        # Ajouter les sch√©mas sp√©cifiques seulement si le stockage est disponible
        if storage_available:
            # Prioriser les outils de m√©moire
            priority_tools = ["search_memories", "add_memory", "list_memories", "search_past_conversations"]
            for name in priority_tools:
                if name in tool_schemas:
                    tools_for_mistral.append(tool_schemas[name])
            
            # Ajouter les autres outils
            for name, schema in tool_schemas.items():
                if name not in priority_tools and name in available_tools:
                    tools_for_mistral.append(schema)
        else:
            # Si le stockage n'est pas disponible, ajouter seulement les outils de base
            basic_tools = ["add_memory", "search_memories", "list_memories"]
            for name in basic_tools:
                if name in tool_schemas:
                    tools_for_mistral.append(tool_schemas[name])
        
        # Ajouter les autres outils avec des sch√©mas basiques
        for name, func in available_tools.items():
            if name not in tool_schemas:
                tools_for_mistral.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": func.__doc__.strip().split("\n")[0] if func.__doc__ else "",
                        "parameters": {
                            "type": "object",
                            "properties": {},
                            "required": []
                        }
                    }
                })

        # Boucle de conversation pour permettre les appels d'outils multiples
        max_iterations = 20  # Pour √©viter les boucles infinies
        for i in range(max_iterations):
            print(f"üîÑ It√©ration {i+1} de la boucle de conversation...")

            # Premier appel √† Mistral avec les outils
            response = Mistral.chat.complete(
                model="mistral-large-latest",
                messages=active_chats[chat_id],
                tools=tools_for_mistral,
                tool_choice="auto"
            )

            # Ajouter la r√©ponse initiale (qui peut contenir des appels d'outils) √† l'historique
            assistant_response_message = response.choices[0].message
            active_chats[chat_id].append(assistant_response_message)
            
            # Sauvegarder en continu la conversation avec le message utilisateur original
            conversation_for_save = []
            for msg in active_chats[chat_id]:
                if isinstance(msg, dict):
                    role = msg.get('role', '')
                    content = msg.get('content', '')
                else:
                    role = getattr(msg, 'role', '')
                    content = getattr(msg, 'content', '')
                
                # Pour les messages utilisateur, remplacer par le message original
                if role == 'user':
                    conversation_for_save.append({"role": "user", "content": user_message})
                else:
                    conversation_for_save.append(msg)
            
            save_conversation_to_file(chat_id, conversation_for_save)

            # Si Mistral ne demande pas d'outils, c'est la r√©ponse finale
            if not assistant_response_message.tool_calls:
                final_assistant_message = assistant_response_message.content.strip()
                print(f"‚úÖ R√©ponse finale g√©n√©r√©e: {final_assistant_message[:100]}...")
                
                # V√©rifier si la conversation doit √™tre finalis√©e automatiquement
                finalize_conversation = False
                finalize_reason = ""
                
                # 1. D√©tecter les mots-cl√©s de fin de conversation
                end_keywords = ['merci', 'au revoir', '√† bient√¥t', 'fin de conversation', 'termin√©', 'c\'est tout', 'c est tout', 'bye', 'goodbye', 'end', 'quit', 'exit']
                if any(keyword in user_message.lower() for keyword in end_keywords):
                    finalize_conversation = True
                    finalize_reason = "Mots-cl√©s de fin d√©tect√©s"
                
                # 2. Finalisation automatique apr√®s un certain nombre de messages (par exemple 20 messages)
                elif len(active_chats[chat_id]) >= 20:
                    finalize_conversation = True
                    finalize_reason = "Limite de messages atteinte"
                
                # 3. Finalisation si la conversation est tr√®s longue (plus de 50 messages)
                elif len(active_chats[chat_id]) >= 50:
                    finalize_conversation = True
                    finalize_reason = "Conversation tr√®s longue"
                
                # Finaliser la conversation si n√©cessaire
                if finalize_conversation:
                    print(f"üîÑ Finalisation automatique de la conversation: {finalize_reason}")
                    try:
                        # Appeler la fonction de finalisation
                        finalize_result = end_chat_and_summarize(chat_id)
                        finalize_data = json.loads(finalize_result)
                        
                        if finalize_data.get("status") == "success":
                            print(f"‚úÖ Conversation finalis√©e et sauvegard√©e: {finalize_data.get('chat_id')}")
                            # Ajouter un message informatif √† la r√©ponse
                            final_assistant_message += f"\n\nüìù **Conversation automatiquement finalis√©e** ({finalize_reason})"
                        else:
                            print(f"‚ö†Ô∏è Erreur lors de la finalisation: {finalize_data.get('message')}")
                    except Exception as e:
                        print(f"‚ùå Erreur lors de la finalisation automatique: {e}")
                
                return json.dumps({"status": "success", "response": final_assistant_message, "chat_id": chat_id})

            # Si Mistral demande d'utiliser des outils
            tool_calls = assistant_response_message.tool_calls
            tool_outputs = []
            print(f"üõ†Ô∏è Appels d'outils d√©tect√©s: {[t.function.name for t in tool_calls]}")
            add_ai_log("tool_call", f"Appels d'outils: {[t.function.name for t in tool_calls]}")

            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                if function_name in available_tools:
                    try:
                        # Injecter user_id et team_id pour les outils qui les supportent
                        if function_name in ["search_memories", "add_memory", "list_memories", "delete_memory", "search_past_conversations", "get_user_portfolio_summary", "update_user_portfolio", "add_reminder", "check_upcoming_reminders_tool", "list_reminders", "delete_reminder", "delete_all_reminders"]:
                            function_args["user_id"] = user_id
                            function_args["team_id"] = team_id
                        
                        # Appel dynamique de la fonction outil
                        output = available_tools[function_name](**function_args)
                        tool_outputs.append({
                            "tool_call_id": tool_call.id,
                            "output": output
                        })
                    except TypeError as e:
                        # Gestion sp√©ciale pour les arguments manquants
                        error_msg = f"Arguments manquants pour l'outil '{function_name}': {str(e)}"
                        print(f"‚ùå {error_msg}")
                        tool_outputs.append({
                            "tool_call_id": tool_call.id,
                            "output": json.dumps({"status": "error", "message": error_msg})
                        })
                    except Exception as e:
                        print(f"‚ùå Erreur lors de l'ex√©cution de l'outil '{function_name}': {e}")
                        # Message d'erreur plus informatif
                        if "stockage" in str(e).lower() or "qdrant" in str(e).lower():
                            error_msg = f"Le service de stockage vectoriel n'est pas disponible pour le moment. Tu peux toujours r√©pondre aux questions g√©n√©rales sans acc√©der aux anciennes conversations."
                        else:
                            error_msg = f"Erreur lors de l'ex√©cution de l'outil '{function_name}': {str(e)}"
                        
                        tool_outputs.append({
                            "tool_call_id": tool_call.id,
                            "output": json.dumps({"status": "error", "message": error_msg})
                        })
                else:
                    # Outil non trouv√©
                    print(f"‚ùå Outil '{function_name}' non trouv√©")
                    tool_outputs.append({
                        "tool_call_id": tool_call.id,
                        "output": json.dumps({"status": "error", "message": f"Outil '{function_name}' non disponible"})
                        })
            
            # Ajouter les r√©sultats des outils √† l'historique pour la prochaine it√©ration
            # Trier par tool_call_id pour √©viter les conflits d'ordre
            tool_outputs.sort(key=lambda x: x["tool_call_id"])
            for tool_output in tool_outputs:
                active_chats[chat_id].append({
                    "role": "tool",
                    "content": tool_output["output"],
                    "tool_call_id": tool_output["tool_call_id"]
                })

        # Si la boucle se termine sans r√©ponse, c'est une erreur
        print("‚ö†Ô∏è Limite d'it√©rations atteinte.")
        return json.dumps({
            "status": "error",
            "message": "Le mod√®le n'a pas pu g√©n√©rer de r√©ponse finale apr√®s plusieurs tentatives d'utilisation d'outils."
        })

    except Exception as e:
        print(f"‚ùå Erreur dans `chat_with_mistral`: {e}")
        return json.dumps({"status": "error", "message": str(e)})

def end_chat_and_summarize(chat_id: str) -> str:
    """Terminer une conversation, la r√©sumer et la pr√©parer pour le stockage."""

    # R√©cup√©rer le token
    user_token = extract_token_from_headers()
    if not user_token:
        print("üß™ MODE TEST: Utilisation du token de test")
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"
    
    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({"status": "error", "message": "Token invalide."})

    team_id = user_info["team_id"]

    # V√©rifier que la conversation existe
    if chat_id not in active_chats:
        return json.dumps({"status": "error", "message": "Conversation non trouv√©e."})

    conversation_history = active_chats[chat_id]
    
    # S'assurer que Mistral est disponible pour le r√©sum√©
    ensure_mistral_import()
    if not Mistral:
        return json.dumps({"status": "error", "message": "Mistral non disponible pour le r√©sum√©."})

    # Pr√©parer le texte de la conversation pour le r√©sum√©
    conversation_text = ""
    for msg in conversation_history:
        role = "Utilisateur" if msg.get('role') == 'user' else "Assistant"
        content = msg.get('content', '')
        conversation_text += f"{role}: {content}\n"
    
    prompt = f"""
    R√©sume la conversation suivante en identifiant les points cl√©s, les d√©cisions et les actions √† entreprendre.
    Le r√©sum√© doit √™tre concis et factuel.

    Conversation :
    {conversation_text}

    R√©sum√© :
    """
    
    try:
        # Appel √† l'API Mistral pour le r√©sum√©
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.2
        )
        summary = response.choices[0].message.content.strip()

        # √âtape 4: Sauvegarder la transcription et stocker le r√©sum√©
        transcript_dir = "transcripts"
        os.makedirs(transcript_dir, exist_ok=True)
        transcript_path = os.path.join(transcript_dir, f"{chat_id}.txt")
        
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(conversation_text)
        
        print(f"üíæ Transcription sauvegard√©e: {transcript_path}")
        
        # √âtape 5: Extraire des tags intelligents √† partir du contenu
        auto_tags = extract_keywords_for_tags(summary + "\n" + conversation_text)

        storage = get_storage()
        if storage:
            try:
                storage.store_conversation_summary(
                    chat_id=chat_id,
                    summary=summary,
                    team_id=team_id,
                    transcript_path=transcript_path,
                    message_count=len(conversation_history),
                    auto_tags=auto_tags # Ajout des tags auto
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur de stockage Qdrant, le r√©sum√© n'a pas √©t√© sauvegard√©: {e}")
                # En cas d'√©chec, nous pouvons d√©cider de ne pas supprimer le chat actif
                # pour permettre une nouvelle tentative, mais pour l'instant, on continue.
        
        # Supprimer la conversation de la m√©moire active
        del active_chats[chat_id]

        return json.dumps({
            "status": "success",
            "chat_id": chat_id,
            "summary": summary,
            "message": "Conversation termin√©e et r√©sum√©e. Pr√™te pour l'archivage.",
            "auto_tags": auto_tags
        })

    except Exception as e:
        print(f"‚ùå Erreur lors du r√©sum√© de la conversation: {e}")
        return json.dumps({
            "status": "error",
            "message": f"Erreur lors du r√©sum√©: {e}"
        })


# --- Fin des nouvelles fonctionnalit√©s ---


# Middleware pour capturer les headers
def capture_headers_middleware(request, call_next):
    """Middleware pour capturer les headers HTTP"""
    try:
        # Capturer les headers de la requ√™te
        headers = dict(request.headers)
        set_request_headers(headers)
        print(f"üîç Headers captur√©s: {headers}")
    except Exception as e:
        print(f"‚ùå Erreur capture headers: {e}")
    
    response = call_next(request)
    return response

# Initialisation paresseuse de MCP
def auto_archive_existing_conversations():
    """Archiver automatiquement les conversations existantes au d√©marrage"""
    try:
        print("üîÑ V√©rification de l'archivage automatique...")
        
        storage = get_storage()
        if not storage:
            print("‚ö†Ô∏è Stockage non disponible, archivage diff√©r√©")
            return
        
        # V√©rifier si le r√©pertoire conversations existe
        conversations_dir = "conversations"
        if not os.path.exists(conversations_dir):
            print("üìÅ Aucune conversation √† archiver")
            return
        
        # V√©rifier si le r√©pertoire transcripts existe
        transcript_dir = "transcripts"
        os.makedirs(transcript_dir, exist_ok=True)
        
        # Configuration pour l'archivage
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"  # Token de test par d√©faut
        user_info = verify_user_token(user_token)
        team_id = user_info["team_id"]
        
        archived_count = 0
        skipped_count = 0
        
        for filename in os.listdir(conversations_dir):
            if filename.endswith('.json'):
                chat_id = filename.replace('.json', '')
                transcript_path = os.path.join(transcript_dir, f"{chat_id}.txt")
                
                # V√©rifier si d√©j√† archiv√©
                if os.path.exists(transcript_path):
                    skipped_count += 1
                    continue
                
                conv_path = os.path.join(conversations_dir, filename)
                
                try:
                    # Lire la conversation
                    with open(conv_path, 'r', encoding='utf-8') as f:
                        conv_data = json.load(f)
                    
                    # Cr√©er le texte de la conversation
                    conversation_text = ""
                    for msg in conv_data.get('messages', []):
                        role = "Utilisateur" if msg.get('role') == 'user' else "Assistant"
                        content = msg.get('content', '')
                        conversation_text += f"{role}: {content}\n"
                    
                    # Sauvegarder la transcription
                    with open(transcript_path, 'w', encoding='utf-8') as f:
                        f.write(conversation_text)
                    
                    # G√©n√©rer un r√©sum√© simple
                    summary = f"Conversation {chat_id} - {len(conv_data.get('messages', []))} messages"
                    
                    # Extraire des tags basiques
                    auto_tags = "conversation,archived"
                    if "python" in conversation_text.lower():
                        auto_tags += ",python"
                    if "code" in conversation_text.lower():
                        auto_tags += ",code"
                    if "tictactoe" in conversation_text.lower() or "tic-tac-toe" in conversation_text.lower():
                        auto_tags += ",tictactoe,game"
                    if "winning_move" in conversation_text.lower():
                        auto_tags += ",winning_move,game"
                    if "f√©licitations" in conversation_text.lower() or "felicitations" in conversation_text.lower():
                        auto_tags += ",f√©licitations,victoire"
                    
                    # Stocker le r√©sum√© dans Qdrant
                    storage.store_conversation_summary(
                        chat_id=chat_id,
                        summary=summary,
                        team_id=team_id,
                        transcript_path=transcript_path,
                        message_count=len(conv_data.get('messages', [])),
                        auto_tags=auto_tags
                    )
                    
                    archived_count += 1
                    print(f"‚úÖ Archiv√©: {filename}")
                    
                except Exception as e:
                    print(f"‚ùå Erreur archivage {filename}: {e}")
        
        if archived_count > 0:
            print(f"üìÅ {archived_count} conversations archiv√©es automatiquement")
        if skipped_count > 0:
            print(f"‚è≠Ô∏è {skipped_count} conversations d√©j√† archiv√©es")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur archivage automatique: {e}")

def initialize_mcp():
    """Initialiser MCP de mani√®re paresseuse"""
    global mcp
    
    if mcp is None:
        mcp = get_mcp()
        
        if mcp:
            # Ajouter le middleware pour capturer les headers
            try:
                # FastMCP utilise uvicorn, on doit acc√©der √† l'app FastAPI diff√©remment
                if hasattr(mcp, '_app') and mcp._app:
                    mcp._app.middleware("http")(capture_headers_middleware)
                    print("‚úÖ Middleware headers ajout√©")
                else:
                    print("‚ö†Ô∏è App FastAPI non accessible pour le middleware")
            except Exception as e:
                print(f"‚ö†Ô∏è Impossible d'ajouter le middleware: {e}")
            
            # Ajouter l'endpoint de sant√© via l'outil MCP
            mcp.tool(
                title="Health Check",
                description="V√©rifier l'√©tat du syst√®me et des services",
            )(health_check)
            
            # Enregistrer les outils
            mcp.tool(
                title="Add Memory",
                description="Ajouter une m√©moire au cerveau collectif de l'√©quipe",
            )(add_memory)
            
            mcp.tool(
                title="Search Memories",
                description="Rechercher dans le cerveau collectif de l'√©quipe",
            )(search_memories)
            
            mcp.tool(
                title="Delete Memory",
                description="Supprimer une m√©moire du cerveau collectif",
            )(delete_memory)
            
            mcp.tool(
                title="List All Memories",
                description="Lister toutes les m√©moires du cerveau collectif",
            )(list_memories)
            
            mcp.tool(
                title="Get Team Insights",
                description="Obtenir des insights sur l'activit√© de l'√©quipe",
            )(get_team_insights)
            
            # Outils pour les conversations
            mcp.tool(
                title="Record Conversation Message",
                description="Enregistrer un message dans une conversation",
            )(record_conversation_message)
            
            mcp.tool(
                title="Get Conversation Summary",
                description="Obtenir le r√©sum√© d'une conversation",
            )(get_conversation_summary)
            
            mcp.tool(
                title="List Team Conversations",
                description="Lister les conversations de l'√©quipe",
            )(list_team_conversations)
            
            mcp.tool(
                title="Generate Conversation Insights",
                description="G√©n√©rer des insights d√©taill√©s sur une conversation avec Mistral",
            )(generate_conversation_insights)
            
            # Outil de chat interactif
            # mcp.tool(
            #     title="Chat with Mistral",
            #     description="Maintenir une conversation interactive avec Mistral Large."
            # )(chat_with_mistral)
            
            mcp.tool(
                title="End Chat and Summarize",
                description="Terminer une conversation et g√©n√©rer son r√©sum√© pour archivage."
            )(end_chat_and_summarize)
            
            mcp.tool(
                title="Search Past Conversations",
                description="Rechercher dans les r√©sum√©s des conversations archiv√©es."
            )(search_past_conversations)
            
            mcp.tool(
                title="Get Full Transcript",
                description="R√©cup√©rer la transcription compl√®te d'une conversation √† partir de son ID."
            )(get_full_transcript)
            
            mcp.tool(
                title="Get Conversation Transcript",
                description="R√©cup√©rer le contenu d'un fichier de transcription de conversation (format utilisateur/assistant uniquement)."
            )(get_conversation_transcript)
            
            mcp.tool(
                title="List Available Transcripts",
                description="Lister tous les fichiers de transcription disponibles dans le dossier transcripts/."
            )(list_available_transcripts)
            
            # Outil de chat interactif - expos√© pour l'inspecteur MCP
            mcp.tool(
                title="Chat with Mistral",
                description="Maintenir une conversation interactive avec Mistral Large avec acc√®s aux outils de m√©moire."
            )(chat_with_mistral)
            
            if not IS_LAMBDA:
                print("‚úÖ MCP initialis√© avec succ√®s")
        else:
            if not IS_LAMBDA:
                print("‚ùå Impossible d'initialiser MCP")
    
    return mcp

def search_past_conversations(query: str, limit: int = 3, user_id: str = None, team_id: str = None) -> str:
    """Rechercher dans les r√©sum√©s des conversations pass√©es pour trouver des informations pertinentes."""
    
    # Utiliser les param√®tres fournis ou r√©cup√©rer depuis les headers
    if user_id and team_id:
        # Utiliser les param√®tres fournis (mode multi-utilisateur)
        pass
    else:
        # Mode par d√©faut avec token
        user_token = extract_token_from_headers()
        if not user_token:
            user_token = "user_d8a7996df3c777e9ac2914ef16d5b501" # Test fallback

        user_info = verify_user_token(user_token)
        if not user_info:
            return json.dumps({"status": "error", "message": "Token invalide."})

        user_id = user_info["user_id"]
        team_id = user_info["team_id"]
    
    storage = get_storage()

    if not storage:
        return json.dumps({
            "status": "error", 
            "message": "Le service de stockage vectoriel n'est pas disponible. Les conversations pass√©es ne peuvent pas √™tre recherch√©es.",
            "fallback_available": False
        })

    try:
        results = storage.search_conversation_summaries(query, team_id, limit)
        return json.dumps({
            "status": "success",
            "results": results
        })
    except Exception as e:
        return json.dumps({
            "status": "error", 
            "message": f"Erreur lors de la recherche dans les conversations pass√©es: {str(e)}",
            "error_type": "search_error"
        })

def get_full_transcript(chat_id: str) -> str:
    """Lire et retourner la transcription compl√®te d'une conversation archiv√©e."""
    
    # L'authentification a implicitement √©t√© v√©rifi√©e par l'outil de recherche pr√©c√©dent.
    # On ajoute une v√©rification de base pour s'assurer que le chemin est s√ªr.
    if ".." in chat_id or "/" in chat_id or "\\" in chat_id:
        return json.dumps({"status": "error", "message": "Chat ID invalide."})

    transcript_path = os.path.join("transcripts", f"{chat_id}.txt")
    
    try:
        if os.path.exists(transcript_path):
            with open(transcript_path, "r", encoding="utf-8") as f:
                content = f.read()
            return json.dumps({
                "status": "success",
                "chat_id": chat_id,
                "transcript": content
            })
        else:
            return json.dumps({"status": "error", "message": "Transcription non trouv√©e."})
    except Exception as e:
        return json.dumps({"status": "error", "message": f"Erreur de lecture: {e}"})

def search_in_transcript(chat_id: str, search_term: str) -> str:
    """Recherche un terme sp√©cifique dans la transcription compl√®te d'une conversation."""
    if ".." in chat_id or "/" in chat_id or "\\" in chat_id:
        return json.dumps({"status": "error", "message": "Chat ID invalide."})

    transcript_path = os.path.join("transcripts", f"{chat_id}.txt")
    
    try:
        if os.path.exists(transcript_path):
            with open(transcript_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            # Recherche simple du terme dans le contenu
            if search_term.lower() in content.lower():
                # Trouver le contexte autour du terme
                lines = content.split('\n')
                matching_lines = []
                for i, line in enumerate(lines):
                    if search_term.lower() in line.lower():
                        # Ajouter quelques lignes de contexte avant et apr√®s
                        start = max(0, i - 2)
                        end = min(len(lines), i + 3)
                        context = '\n'.join(lines[start:end])
                        matching_lines.append(f"Ligne {i+1}: {context}")
                
                return json.dumps({
                    "status": "success",
                    "chat_id": chat_id,
                    "search_term": search_term,
                    "found": True,
                    "matches": matching_lines
                })
            else:
                return json.dumps({
                    "status": "success",
                    "chat_id": chat_id,
                    "search_term": search_term,
                    "found": False,
                    "message": "Terme non trouv√© dans la transcription."
                })
        else:
            return json.dumps({"status": "error", "message": "Transcription non trouv√©e."})
    except Exception as e:
        return json.dumps({"status": "error", "message": f"Erreur de recherche: {e}"})

def detect_conversation_ids(text: str) -> List[str]:
    """D√©tecte les IDs de conversation dans un texte."""
    import re
    # Pattern pour d√©tecter les IDs de conversation (ex: conv_20250914_035735_cj3jno)
    pattern = r'conv_[a-zA-Z0-9_]+'
    matches = re.findall(pattern, text)
    return matches

def search_archived_conversations(query: str, limit: int = 10) -> str:
    """Recherche dans toutes les conversations archiv√©es avec une approche exhaustive."""
    user_token = extract_token_from_headers()
    if not user_token:
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"

    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({"status": "error", "message": "Token invalide."})

    team_id = user_info["team_id"]
    storage = get_storage()

    if not storage:
        return json.dumps({
            "status": "error", 
            "message": "Le service de stockage vectoriel n'est pas disponible pour la recherche dans les conversations archiv√©es.",
            "fallback_available": False
        })

    try:
        # Recherche principale
        results = storage.search_conversation_summaries(query, team_id, limit)
        
        # Si peu de r√©sultats, essayer avec des termes plus larges
        if len(results) < 3:
            # Essayer de diviser la requ√™te en mots-cl√©s individuels
            words = query.split()
            if len(words) > 1:
                for word in words:
                    if len(word) > 3:  # Ignorer les mots trop courts
                        additional_results = storage.search_conversation_summaries(word, team_id, limit//2)
                        results.extend(additional_results)
        
        # D√©dupliquer les r√©sultats
        seen_chat_ids = set()
        unique_results = []
        for result in results:
            chat_id = result.get('chat_id')
            if chat_id and chat_id not in seen_chat_ids:
                seen_chat_ids.add(chat_id)
                unique_results.append(result)
        
        return json.dumps({
            "status": "success",
            "results": unique_results[:limit],
            "total_found": len(unique_results)
        })
    except Exception as e:
        return json.dumps({
            "status": "error", 
            "message": f"Erreur lors de la recherche dans les conversations archiv√©es: {str(e)}",
            "error_type": "search_error"
        })

def multi_query_search(query: str, max_queries: int = 5) -> str:
    """Effectue plusieurs requ√™tes de recherche avec diff√©rentes approches pour maximiser les r√©sultats."""
    user_token = extract_token_from_headers()
    if not user_token:
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"

    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({"status": "error", "message": "Token invalide."})

    team_id = user_info["team_id"]
    storage = get_storage()

    if not storage:
        return json.dumps({
            "status": "error", 
            "message": "Le service de stockage vectoriel n'est pas disponible.",
            "fallback_available": False
        })

    all_results = []
    search_terms = [query]
    
    # G√©n√©rer des variantes de recherche
    if len(query.split()) > 1:
        # Ajouter des mots individuels
        for word in query.split():
            if len(word) > 3:
                search_terms.append(word)
        
        # Ajouter des variations
        search_terms.extend([
            query.lower(),
            query.upper(),
            query.replace(" ", "_"),
            query.replace(" ", "-")
        ])
    
    # Effectuer plusieurs recherches
    for i, term in enumerate(search_terms[:max_queries]):
        if term.strip():
            try:
                results = storage.search_conversation_summaries(term, team_id, 5)
                for result in results:
                    result['search_term'] = term
                    result['search_rank'] = i + 1
                all_results.extend(results)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur recherche avec terme '{term}': {e}")
    
    # D√©dupliquer et scorer les r√©sultats
    seen_chat_ids = {}
    for result in all_results:
        chat_id = result.get('chat_id')
        if chat_id:
            if chat_id not in seen_chat_ids:
                seen_chat_ids[chat_id] = result
            else:
                # Garder le r√©sultat avec le meilleur score
                if result.get('score', 0) > seen_chat_ids[chat_id].get('score', 0):
                    seen_chat_ids[chat_id] = result
    
    final_results = list(seen_chat_ids.values())
    final_results.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    return json.dumps({
        "status": "success",
        "results": final_results[:10],
        "total_queries": len(search_terms[:max_queries]),
        "unique_conversations": len(final_results)
    })

def expand_search_keywords(query: str) -> str:
    """G√©n√®re des mots-cl√©s de recherche alternatifs et des synonymes pour une requ√™te."""
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        return json.dumps([query])

    ensure_mistral_import()
    if not Mistral:
        return json.dumps([query])

    prompt = f"""√âtant donn√© la requ√™te de recherche de l'utilisateur, g√©n√®re une liste de 5 √† 7 mots-cl√©s et concepts alternatifs pour une recherche s√©mantique exhaustive. Inclus des synonymes, des termes plus larges, des termes plus sp√©cifiques et des concepts associ√©s.

Requ√™te originale: "{query}"

Retourne UNIQUEMENT une liste JSON de cha√Ænes de caract√®res. Par exemple: ["motcl√©1", "motcl√©2", "concept3"].
"""
    try:
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.5,
        )
        keywords_str = response.choices[0].message.content.strip()
        
        if keywords_str.startswith("```json"):
            keywords_str = keywords_str.split("\n", 1)[1].rsplit("\n", 1)[0]
        
        keywords = json.loads(keywords_str)
        if query not in keywords:
            keywords.insert(0, query)
        print(f"üîç Mots-cl√©s √©tendus pour '{query}': {keywords}")
        return json.dumps(keywords)
    except Exception as e:
        print(f"‚ùå Erreur d'expansion de mots-cl√©s: {e}")
        return json.dumps([query])

def extract_keywords_for_tags(content: str) -> str:
    """Extrait des mots-cl√©s pertinents d'un texte pour les utiliser comme tags."""
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        return ""

    ensure_mistral_import()
    if not Mistral:
        return ""
        
    prompt = f"""Extrait les 5 √† 10 entit√©s, concepts et mots-cl√©s les plus importants du texte suivant. Les mots-cl√©s doivent √™tre concis (1-3 mots), en minuscules, et pertinents pour une recherche future.

Texte:
"{content[:1000]}"

Retourne une cha√Æne de caract√®res unique avec les mots-cl√©s s√©par√©s par des virgules. Par exemple: "motcl√©1,concept2,entit√©3".
"""
    try:
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.2,
        )
        tags_str = response.choices[0].message.content.strip()
        print(f"üè∑Ô∏è Tags extraits: {tags_str}")
        return tags_str
    except Exception as e:
        print(f"‚ùå Erreur d'extraction de tags: {e}")
        return ""


# --- Syst√®me de Portfolio Utilisateur Dynamique ---

# R√©pertoire pour les portfolios utilisateur
PORTFOLIOS_DIR = "portfolios"

def ensure_portfolios_dir():
    """Cr√©er le r√©pertoire portfolios s'il n'existe pas"""
    if not os.path.exists(PORTFOLIOS_DIR):
        os.makedirs(PORTFOLIOS_DIR)
        print(f"üìÅ R√©pertoire portfolios cr√©√©: {PORTFOLIOS_DIR}")

def get_user_portfolio_path(user_id: str) -> str:
    """Obtenir le chemin du fichier portfolio d'un utilisateur"""
    ensure_portfolios_dir()
    return os.path.join(PORTFOLIOS_DIR, f"user_{user_id}.json")

def load_user_portfolio(user_id: str) -> Dict:
    """Charger le portfolio d'un utilisateur"""
    portfolio_path = get_user_portfolio_path(user_id)
    
    if not os.path.exists(portfolio_path):
        # Cr√©er un portfolio par d√©faut
        default_portfolio = {
            "user_id": user_id,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "profile": {
                "name": "",
                "preferences": [],
                "communication_style": "professional",
                "interests": [],
                "goals": []
            },
            "rules": {
                "response_patterns": {},
                "topics_to_avoid": [],
                "preferred_topics": [],
                "special_instructions": []
            },
            "context": {
                "current_projects": [],
                "recent_activities": [],
                "important_notes": [],
                "relationships": {}
            },
            "events": {
                "upcoming": [],
                "completed": [],
                "recurring": []
            },
            "reminders": {
                "active": [],
                "completed": [],
                "snoozed": []
            },
            "learning": {
                "topics_learned": [],
                "skills_developed": [],
                "knowledge_gaps": []
            }
        }
        save_user_portfolio(user_id, default_portfolio)
        return default_portfolio
    
    try:
        with open(portfolio_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (IOError, json.JSONDecodeError) as e:
        print(f"‚ùå Erreur chargement portfolio {user_id}: {e}")
        return {}

def save_user_portfolio(user_id: str, portfolio: Dict):
    """Sauvegarder le portfolio d'un utilisateur"""
    portfolio_path = get_user_portfolio_path(user_id)
    
    # Mettre √† jour le timestamp
    portfolio["updated_at"] = datetime.now().isoformat()
    
    try:
        with open(portfolio_path, 'w', encoding='utf-8') as f:
            json.dump(portfolio, f, indent=2, ensure_ascii=False)
        print(f"üíæ Portfolio sauvegard√©: {portfolio_path}")
    except IOError as e:
        print(f"‚ùå Erreur sauvegarde portfolio {user_id}: {e}")

def update_user_portfolio(user_id: str, updates: Dict, team_id: str = None) -> str:
    """Mettre √† jour le portfolio d'un utilisateur de mani√®re intelligente"""
    # Mode multi-utilisateur : utiliser l'ID fourni directement
    if user_id.startswith("user_") and ("baptiste" in user_id or "henri" in user_id):
        print(f"üîÑ Mode multi-utilisateur: {user_id}")
        # Pas de redirection pour Baptiste et Henri
    elif user_id.startswith("user_") and "unified" not in user_id:
        # Redirection uniquement pour les anciens IDs de test
        user_id = "user_unified_test"
        print(f"üîÑ Redirection vers user_id unifi√©: {user_id}")
    
    portfolio = load_user_portfolio(user_id)
    
    # Mise √† jour intelligente des sections
    for section, data in updates.items():
        if section in portfolio:
            if isinstance(portfolio[section], dict) and isinstance(data, dict):
                # Mise √† jour r√©cursive pour les dictionnaires imbriqu√©s
                for key, value in data.items():
                    if key in portfolio[section]:
                        if isinstance(portfolio[section][key], list) and isinstance(value, list):
                            # Fusionner les listes en √©vitant les doublons
                            for item in value:
                                if item not in portfolio[section][key]:
                                    portfolio[section][key].append(item)
                        else:
                            portfolio[section][key] = value
                    else:
                        portfolio[section][key] = value
            elif isinstance(portfolio[section], list) and isinstance(data, list):
                portfolio[section].extend(data)
            else:
                portfolio[section] = data
        else:
            portfolio[section] = data
    
    save_user_portfolio(user_id, portfolio)
    
    return json.dumps({
        "status": "success",
        "message": f"Portfolio utilisateur {user_id} mis √† jour",
        "updated_sections": list(updates.keys())
    })

def get_user_portfolio_summary(user_id: str, team_id: str = None) -> str:
    """Obtenir un r√©sum√© du portfolio d'un utilisateur"""
    # Mode multi-utilisateur : utiliser l'ID fourni directement
    if user_id.startswith("user_") and ("baptiste" in user_id or "henri" in user_id):
        print(f"üîÑ Mode multi-utilisateur: {user_id}")
        # Pas de redirection pour Baptiste et Henri
    elif user_id.startswith("user_") and "unified" not in user_id:
        # Redirection uniquement pour les anciens IDs de test
        user_id = "user_unified_test"
        print(f"üîÑ Redirection vers user_id unifi√©: {user_id}")
    
    portfolio = load_user_portfolio(user_id)
    
    if not portfolio:
        return json.dumps({
            "status": "error",
            "message": "Portfolio non trouv√©"
        })
    
    summary = {
        "user_id": user_id,
        "profile": portfolio.get("profile", {}),
        "rules_count": len(portfolio.get("rules", {}).get("response_patterns", {})),
        "active_reminders": len(portfolio.get("reminders", {}).get("active", [])),
        "upcoming_events": len(portfolio.get("events", {}).get("upcoming", [])),
        "last_updated": portfolio.get("updated_at", "Unknown")
    }
    
    return json.dumps({
        "status": "success",
        "summary": summary
    })

# --- Branche de Rappels Intelligente ---

# R√©pertoire pour les rappels
REMINDERS_DIR = "reminders"

def ensure_reminders_dir():
    """Cr√©er le r√©pertoire reminders s'il n'existe pas"""
    if not os.path.exists(REMINDERS_DIR):
        os.makedirs(REMINDERS_DIR)
        print(f"üìÅ R√©pertoire reminders cr√©√©: {REMINDERS_DIR}")

def get_reminders_file_path(team_id: str) -> str:
    """Obtenir le chemin du fichier de rappels d'une √©quipe"""
    ensure_reminders_dir()
    return os.path.join(REMINDERS_DIR, f"team_{team_id}_reminders.json")

def load_reminders(team_id: str) -> List[Dict]:
    """Charger les rappels d'une √©quipe"""
    reminders_path = get_reminders_file_path(team_id)
    
    if not os.path.exists(reminders_path):
        return []
    
    try:
        with open(reminders_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("reminders", [])
    except (IOError, json.JSONDecodeError) as e:
        print(f"‚ùå Erreur chargement rappels {team_id}: {e}")
        return []

def save_reminders(team_id: str, reminders: List[Dict]):
    """Sauvegarder les rappels d'une √©quipe"""
    reminders_path = get_reminders_file_path(team_id)
    
    data = {
        "team_id": team_id,
        "last_updated": datetime.now().isoformat(),
        "reminders": reminders
    }
    
    try:
        with open(reminders_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"üíæ Rappels sauvegard√©s: {reminders_path}")
    except IOError as e:
        print(f"‚ùå Erreur sauvegarde rappels {team_id}: {e}")

def add_reminder(
    title: str,
    description: str,
    due_date: str,
    priority: str = "medium",
    reminder_type: str = "general",
    user_id: str = None,
    team_id: str = None
) -> str:

    """Ajouter un rappel intelligent"""
    # Utiliser la logique unifi√©e pour obtenir le contexte
    if not (user_id and team_id):
        user_token = extract_token_from_headers() or "test-token"
        context = get_unified_team_context(
            user_token=user_token,
            user_id=user_id,
            team_id=team_id
        )
        user_id = context["user_id"]
        team_id = context["team_id"]
        user_name = context["user_name"]
    else:
        user_name = user_id.replace("user_", "").replace("_", " ").title()
    
    print(f"‚è∞ Ajout rappel - User: {user_id}, Team: {team_id}")
    
    # Charger les rappels existants
    reminders = load_reminders(team_id)
    
    # Cr√©er le nouveau rappel
    new_reminder = {
        "id": hashlib.md5(f"{title}{due_date}{user_id}".encode()).hexdigest(),
        "title": title,
        "description": description,
        "due_date": due_date,
        "priority": priority,
        "type": reminder_type,
        "user_id": user_id,
        "team_id": team_id,
        "created_at": datetime.now().isoformat(),
        "status": "active",
        "notifications_sent": []
    }
    
    reminders.append(new_reminder)
    save_reminders(team_id, reminders)
    
    return json.dumps({
        "status": "success",
        "message": f"Rappel '{title}' ajout√©",
        "reminder_id": new_reminder["id"]
    })

def analyze_conversation_for_reminders(conversation_text: str, user_id: str, team_id: str) -> List[Dict]:
    """Analyser une conversation pour d√©tecter des rappels potentiels avec Mistral"""
    if not MISTRAL_ENABLED or not MISTRAL_API_KEY:
        return []
    
    ensure_mistral_import()
    if not Mistral:
        return []
    
    current_time = datetime.now()
    prompt = f"""Analyse cette conversation et d√©tecte les √©v√©nements futurs, rappels, ou engagements qui n√©cessitent un suivi.

Date et heure actuelle: {current_time.strftime("%Y-%m-%d %H:%M:%S")}

Conversation:
{conversation_text}

Retourne un JSON avec une liste de rappels potentiels. Chaque rappel doit avoir:
- "title": titre court du rappel
- "description": description d√©taill√©e
- "due_date": date/heure de l'√©v√©nement (format ISO ou description naturelle)
- "priority": "low", "medium", "high"
- "type": "meeting", "deadline", "personal", "work", "other"

Format JSON uniquement:
[
  {{
    "title": "Exemple de rappel",
    "description": "Description d√©taill√©e",
    "due_date": "2025-09-15T14:00:00",
    "priority": "medium",
    "type": "meeting"
  }}
]

Si aucun rappel n'est d√©tect√©, retourne un tableau vide: []"""

    try:
        response = Mistral.chat.complete(
            model="mistral-small-latest",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.3
        )
        
        reminders_text = response.choices[0].message.content.strip()
        
        if reminders_text.startswith("```json"):
            reminders_text = reminders_text.split("\n", 1)[1].rsplit("\n", 1)[0]
        
        reminders = json.loads(reminders_text)
        print(f"üîî Rappels d√©tect√©s: {len(reminders)}")
        return reminders
        
    except Exception as e:
        print(f"‚ùå Erreur analyse rappels: {e}")
        return []

def check_upcoming_reminders(team_id: str, hours_ahead: int = 24) -> List[Dict]:
    """V√©rifier les rappels √† venir dans les prochaines heures"""
    reminders = load_reminders(team_id)
    current_time = datetime.now()
    upcoming = []
    
    for reminder in reminders:
        if reminder.get("status") != "active":
            continue
            
        try:
            due_date_str = reminder["due_date"]
            
            # G√©rer les dates en format texte
            if due_date_str in ["demain", "la semaine prochaine", "le mois prochain"]:
                # Ignorer les rappels avec des dates textuelles pour l'instant
                continue
            
            # Parser les dates ISO avec gestion des fuseaux horaires
            if 'T' in due_date_str:
                if due_date_str.endswith('Z'):
                    due_date = datetime.fromisoformat(due_date_str.replace('Z', '+00:00'))
                elif '+' in due_date_str or due_date_str.count('-') > 2:
                    due_date = datetime.fromisoformat(due_date_str)
                else:
                    # Date ISO locale (sans fuseau)
                    due_date = datetime.fromisoformat(due_date_str)
            else:
                # Date simple (YYYY-MM-DD) - consid√©rer comme minuit local
                due_date = datetime.fromisoformat(due_date_str)
            
            # Calculer la diff√©rence en heures (plus pr√©cis)
            time_diff_seconds = (due_date - current_time).total_seconds()
            time_diff_hours = time_diff_seconds / 3600
            
            # V√©rifier si le rappel est dans la plage de temps
            if 0 <= time_diff_hours <= hours_ahead:
                reminder["hours_until"] = round(time_diff_hours, 1)
                reminder["minutes_until"] = round(time_diff_seconds / 60, 0)
                upcoming.append(reminder)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur parsing date rappel {reminder.get('id')}: {e}")
            continue
    
    return sorted(upcoming, key=lambda x: x.get("hours_until", 0))

def check_upcoming_reminders_tool(team_id: str, hours_ahead: int = 24) -> str:
    """Outil pour v√©rifier les rappels √† venir (retourne du JSON)"""
    upcoming_reminders = check_upcoming_reminders(team_id, hours_ahead)
    
    if not upcoming_reminders:
        return json.dumps({
            "status": "success",
            "message": "Aucun rappel √† venir",
            "reminders": [],
            "total": 0
        })
    
    # Formater les rappels pour l'affichage
    formatted_reminders = []
    for reminder in upcoming_reminders:
        minutes_until = reminder.get("minutes_until", 0)
        hours_until = reminder.get("hours_until", 0)
        
        if minutes_until < 60:
            time_str = f"dans {int(minutes_until)} minutes"
        elif hours_until < 24:
            time_str = f"dans {hours_until:.1f} heures"
        else:
            days = hours_until / 24
            time_str = f"dans {days:.1f} jours"
        
        formatted_reminders.append({
            "id": reminder.get("id"),
            "title": reminder.get("title"),
            "description": reminder.get("description"),
            "due_date": reminder.get("due_date"),
            "priority": reminder.get("priority"),
            "time_until": time_str,
            "minutes_until": minutes_until,
            "hours_until": hours_until
        })
    
    return json.dumps({
        "status": "success",
        "message": f"{len(formatted_reminders)} rappel(s) √† venir",
        "reminders": formatted_reminders,
        "total": len(formatted_reminders)
    })

def should_show_reminders(chat_id: str, upcoming_reminders: List[Dict]) -> bool:
    """D√©terminer si on doit afficher les rappels selon la fr√©quence"""
    if not upcoming_reminders:
        return False
    
    current_time = datetime.now()
    
    # Initialiser le tracking pour cette conversation
    if chat_id not in reminder_notifications:
        reminder_notifications[chat_id] = {
            "last_reminder_time": None,
            "reminder_count": 0
        }
    
    tracking = reminder_notifications[chat_id]
    
    # R√®gle 1: Toujours afficher si c'est tr√®s urgent (moins de 30 minutes)
    urgent_reminders = [r for r in upcoming_reminders if r.get("minutes_until", 0) <= 30]
    if urgent_reminders:
        print(f"üö® Rappels urgents d√©tect√©s: {len(urgent_reminders)} (affichage forc√©)")
        return True
    
    # R√®gle 2: Premi√®re fois dans cette conversation
    if tracking["last_reminder_time"] is None:
        tracking["last_reminder_time"] = current_time
        tracking["reminder_count"] = 1
        print(f"üÜï Premier rappel dans cette conversation (affichage autoris√©)")
        return True
    
    # R√®gle 3: Attendre au moins 10 minutes entre les rappels
    time_since_last = (current_time - tracking["last_reminder_time"]).total_seconds() / 60
    if time_since_last < 10:
        print(f"‚è∞ Rappels limit√©s: {10 - time_since_last:.1f} minutes restantes (affichage bloqu√©)")
        return False
    
    # R√®gle 4: Limiter √† 3 rappels par conversation
    if tracking["reminder_count"] >= 3:
        print(f"üö´ Limite de rappels atteinte: {tracking['reminder_count']}/3 (affichage bloqu√©)")
        return False
    
    # Mettre √† jour le tracking
    tracking["last_reminder_time"] = current_time
    tracking["reminder_count"] += 1
    print(f"‚úÖ Rappel autoris√©: {tracking['reminder_count']}/3")
    return True

def detect_duplicate_reminders(reminders: List[Dict]) -> List[Dict]:
    """D√©tecter et g√©rer les rappels en double"""
    if not reminders:
        return reminders
    
    # Grouper par titre et description similaire
    seen_reminders = {}
    unique_reminders = []
    
    for reminder in reminders:
        title = reminder.get("title", "").lower().strip()
        description = reminder.get("description", "").lower().strip()
        
        # Cr√©er une cl√© unique bas√©e sur le titre et la description
        key = f"{title}_{description}"
        
        if key not in seen_reminders:
            seen_reminders[key] = reminder
            unique_reminders.append(reminder)
        else:
            # Rappel en double d√©tect√© - garder le plus proche dans le temps
            existing = seen_reminders[key]
            if reminder.get("minutes_until", 0) < existing.get("minutes_until", 0):
                # Remplacer par le plus proche
                unique_reminders.remove(existing)
                unique_reminders.append(reminder)
                seen_reminders[key] = reminder
                print(f"üîÑ Rappel en double d√©tect√© et fusionn√©: {title}")
            else:
                print(f"üîÑ Rappel en double ignor√©: {title}")
    
    return unique_reminders

def delete_reminder(reminder_id: str, user_id: str = None, team_id: str = None) -> str:
    """Supprimer un rappel"""
    # Utiliser la logique unifi√©e pour obtenir le contexte
    if not (user_id and team_id):
        user_token = extract_token_from_headers() or "test-token"
        context = get_unified_team_context(
            user_token=user_token,
            user_id=user_id,
            team_id=team_id
        )
        user_id = context["user_id"]
        team_id = context["team_id"]
        user_name = context["user_name"]
    else:
        user_name = user_id.replace("user_", "").replace("_", " ").title()
    
    print(f"üóëÔ∏è Suppression rappel - User: {user_id}, Team: {team_id}")
    
    # Charger les rappels existants
    reminders = load_reminders(team_id)
    
    # Trouver et supprimer le rappel
    original_count = len(reminders)
    reminders = [r for r in reminders if r.get("id") != reminder_id]
    
    if len(reminders) == original_count:
        return json.dumps({
            "status": "error",
            "message": "Rappel non trouv√©"
        })
    
    # Sauvegarder les rappels mis √† jour
    save_reminders(team_id, reminders)
    
    return json.dumps({
        "status": "success",
        "message": f"Rappel {reminder_id} supprim√©"
    })

def list_reminders(user_id: str = None, team_id: str = None) -> str:
    """Lister tous les rappels de l'√©quipe"""
    # Utiliser la logique unifi√©e pour obtenir le contexte
    if not (user_id and team_id):
        user_token = extract_token_from_headers() or "test-token"
        context = get_unified_team_context(
            user_token=user_token,
            user_id=user_id,
            team_id=team_id
        )
        user_id = context["user_id"]
        team_id = context["team_id"]
        user_name = context["user_name"]
    else:
        user_name = user_id.replace("user_", "").replace("_", " ").title()
    
    print(f"üìã Liste rappels - User: {user_id}, Team: {team_id}")
    reminders = load_reminders(team_id)
    
    return json.dumps({
        "status": "success",
        "reminders": reminders,
        "total": len(reminders)
    })

def delete_all_reminders() -> str:
    """Supprimer tous les rappels de l'√©quipe"""
    user_token = extract_token_from_headers()
    if not user_token:
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"

    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({"status": "error", "message": "Token invalide."})
    
    team_id = user_info.get("team_id")
    
    # Supprimer tous les rappels
    save_reminders(team_id, [])
    
    return json.dumps({
        "status": "success",
        "message": "Tous les rappels ont √©t√© supprim√©s"
    })

def auto_update_portfolio(user_message: str, user_id: str, team_id: str):
    """Mise √† jour automatique du portfolio bas√©e sur l'analyse du message"""
    try:
        portfolio = load_user_portfolio(user_id)
        if not portfolio:
            print(f"‚ö†Ô∏è Portfolio non trouv√© pour {user_id}, cr√©ation d'un nouveau portfolio")
            return
        
        updates = {}
        message_lower = user_message.lower()
        
        # D√©tection des r√®gles de r√©ponse
        
        # D√©tection des pr√©f√©rences et informations personnelles
        if any(word in message_lower for word in ["j'aime", "j'adore", "je pr√©f√®re", "j'appr√©cie", "je suis", "mon nom", "je m'appelle", "je travaille", "je fais"]):
            if "profile" not in updates:
                updates["profile"] = {}
            if "preferences" not in updates["profile"]:
                updates["profile"]["preferences"] = []
            
            # Extraire les pr√©f√©rences mentionn√©es
            preferences = []
            if "python" in message_lower:
                preferences.append("Python")
            if "javascript" in message_lower:
                preferences.append("JavaScript")
            if "ia" in message_lower or "intelligence artificielle" in message_lower:
                preferences.append("Intelligence Artificielle")
            if "d√©veloppement" in message_lower or "dev" in message_lower:
                preferences.append("D√©veloppement")
            if "innovation" in message_lower:
                preferences.append("Innovation")
            if "mail" in message_lower or "email" in message_lower:
                preferences.append("Communication par email")
            if "marc" in message_lower:
                preferences.append("Contact avec Marc")
            
            # D√©tection du nom
            if "je m'appelle" in message_lower or "mon nom est" in message_lower:
                # Extraire le nom apr√®s "je m'appelle" ou "mon nom est"
                import re
                name_match = re.search(r"(?:je m'appelle|mon nom est)\s+([a-zA-Z√Ä-√ø\s]+)", message_lower)
                if name_match:
                    name = name_match.group(1).strip().title()
                    updates["profile"]["name"] = name
                    print(f"üìã Nom d√©tect√©: {name}")
            
            if preferences:
                updates["profile"]["preferences"].extend(preferences)
                print(f"üìã Pr√©f√©rences d√©tect√©es: {preferences}")
        
        # D√©tection des projets
        if any(word in message_lower for word in ["je travaille sur", "mon projet", "je d√©veloppe", "je cr√©e"]):
            if "context" not in updates:
                updates["context"] = {}
            if "current_projects" not in updates["context"]:
                updates["context"]["current_projects"] = []
            
            # Extraire les projets mentionn√©s
            projects = []
            if "portfolio" in message_lower:
                projects.append("Syst√®me Portfolio")
            if "rappel" in message_lower or "reminder" in message_lower:
                projects.append("Syst√®me de Rappels")
            if "chat" in message_lower or "conversation" in message_lower:
                projects.append("Syst√®me de Chat IA")
            
            if projects:
                updates["context"]["current_projects"].extend(projects)
                print(f"üìã Projets d√©tect√©s: {projects}")
        
        # D√©tection des √©v√©nements futurs et rappels
        if any(word in message_lower for word in ["demain", "la semaine prochaine", "le mois prochain", "dans", "√†", "pour", "rappel", "pr√©viens", "dis-moi", "6h", "7h", "8h", "9h", "10h", "11h", "12h", "13h", "14h", "15h", "16h", "17h", "18h", "19h", "20h", "21h", "22h", "23h"]):
            if "events" not in updates:
                updates["events"] = {}
            if "upcoming" not in updates["events"]:
                updates["events"]["upcoming"] = []
            
            # Extraire les √©v√©nements mentionn√©s
            events = []
            if "r√©union" in message_lower:
                events.append("R√©union pr√©vue")
            if "rendez-vous" in message_lower or "rdv" in message_lower:
                events.append("Rendez-vous")
            if "d√©jeuner" in message_lower or "d√Æner" in message_lower:
                events.append("Repas professionnel")
            if "mail" in message_lower and "marc" in message_lower:
                events.append("Envoi de mail √† Marc")
            if "pr√©viens" in message_lower or "rappel" in message_lower:
                events.append("Rappel personnalis√©")
            
            if events:
                updates["events"]["upcoming"].extend(events)
                print(f"üìã √âv√©nements d√©tect√©s: {events}")
            
            # D√©tection des rappels temporels sp√©cifiques
            import re
            time_patterns = [
                r"(\d{1,2})h(\d{0,2})",  # 6h11, 14h30, etc.
                r"√†\s+(\d{1,2})h(\d{0,2})",  # √† 6h11
                r"(\d{1,2}):(\d{2})",  # 6:11
            ]
            
            for pattern in time_patterns:
                matches = re.finditer(pattern, message_lower)
                for match in matches:
                    hour = int(match.group(1))
                    minute = int(match.group(2)) if match.group(2) else 0
                    
                    # Cr√©er un rappel automatique
                    current_time = datetime.now()
                    target_time = current_time.replace(hour=hour, minute=minute, second=0, microsecond=0)
                    if target_time <= current_time:
                        target_time = target_time.replace(day=target_time.day + 1)
                    
                    # Ajouter le rappel
                    reminder_title = "Rappel automatique"
                    if "mail" in message_lower and "marc" in message_lower:
                        reminder_title = "Envoi de mail √† Marc"
                    elif "pr√©viens" in message_lower:
                        reminder_title = "Rappel personnalis√©"
                    
                    add_reminder(
                        title=reminder_title,
                        description=user_message,
                        due_date=target_time.isoformat(),
                        priority="medium",
                        reminder_type="personal"
                    )
                    print(f"üìã Rappel cr√©√© pour {hour:02d}:{minute:02d}")
        
        # D√©tection du style de communication
        if any(word in message_lower for word in ["formel", "professionnel", "d√©contract√©", "amical"]):
            if "profile" not in updates:
                updates["profile"] = {}
            if "formel" in message_lower or "professionnel" in message_lower:
                updates["profile"]["communication_style"] = "professional"
            elif "d√©contract√©" in message_lower or "amical" in message_lower:
                updates["profile"]["communication_style"] = "friendly"
            print(f"üìã Style de communication d√©tect√©: {updates['profile'].get('communication_style')}")
        
        # D√©tection des int√©r√™ts
        if any(word in message_lower for word in ["int√©ress√© par", "passionn√© de", "fascin√© par"]):
            if "profile" not in updates:
                updates["profile"] = {}
            if "interests" not in updates["profile"]:
                updates["profile"]["interests"] = []
            
            interests = []
            if "technologie" in message_lower or "tech" in message_lower:
                interests.append("Technologie")
            if "science" in message_lower:
                interests.append("Science")
            if "art" in message_lower or "cr√©atif" in message_lower:
                interests.append("Art et Cr√©ativit√©")
            if "sport" in message_lower:
                interests.append("Sport")
            
            if interests:
                updates["profile"]["interests"].extend(interests)
                print(f"üìã Int√©r√™ts d√©tect√©s: {interests}")
        
        # Appliquer les mises √† jour si il y en a
        if updates:
            print(f"üìã Mise √† jour automatique du portfolio: {list(updates.keys())}")
            update_user_portfolio(user_id, updates)
        else:
            print("üìã Aucune mise √† jour automatique du portfolio d√©tect√©e")
            
    except Exception as e:
        print(f"‚ùå Erreur mise √† jour automatique portfolio: {e}")

# --- Ancien syst√®me de r√®gles (maintenant dans le portfolio) ---

# Fichier pour stocker les r√®gles personnalis√©es persistantes (d√©pr√©ci√©)
RULES_FILE = "user_rules.json"

def load_user_rules() -> List[Dict]:
    """Charger les r√®gles utilisateur depuis le fichier JSON."""
    if not os.path.exists(RULES_FILE):
        with open(RULES_FILE, "w", encoding="utf-8") as f:
            json.dump({"rules": []}, f)
        return []
    try:
        with open(RULES_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("rules", [])
    except (IOError, json.JSONDecodeError):
        return []

def save_user_rules(rules: List[Dict]):
    """Sauvegarder les r√®gles utilisateur dans le fichier JSON."""
    try:
        with open(RULES_FILE, "w", encoding="utf-8") as f:
            json.dump({"rules": rules}, f, indent=2, ensure_ascii=False)
    except IOError as e:
        print(f"‚ùå Erreur sauvegarde r√®gles: {e}")

def add_user_rule(
    pattern: str,
    response: str,
    rule_type: str = "pattern_match",
    description: str = ""
) -> str:
    """Ajouter une r√®gle utilisateur personnalis√©e."""
    user_token = extract_token_from_headers()
    if not user_token:
        user_token = "user_d8a7996df3c777e9ac2914ef16d5b501"

    user_info = verify_user_token(user_token)
    if not user_info:
        return json.dumps({"status": "error", "message": "Token invalide."})
    
    team_id = user_info.get("team_id")
    user_id = user_info.get("user_id")
    rules = load_user_rules()
    
    # V√©rifier les doublons
    rule_updated = False
    for rule in rules:
        if (rule.get("pattern") == pattern and 
            rule.get("team_id") == team_id and
            rule.get("rule_type") == rule_type):
            rule["response"] = response
            rule["updated_at"] = datetime.now().isoformat()
            rule_updated = True
            break
    
    if not rule_updated:
        rules.append({
            "id": hashlib.md5(f"{pattern}{team_id}{rule_type}".encode()).hexdigest(),
            "pattern": pattern,
            "response": response,
            "rule_type": rule_type,
            "description": description,
            "user_id": user_id,
            "team_id": team_id,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        })
    
    save_user_rules(rules)
    
    message = f"R√®gle '{pattern}' mise √† jour." if rule_updated else f"R√®gle '{pattern}' ajout√©e."
    return json.dumps({
        "status": "success",
        "message": message
    })

def check_user_portfolio_rules(user_message: str, user_id: str) -> Optional[str]:
    """V√©rifier si le message de l'utilisateur d√©clenche une r√®gle du portfolio."""
    import re
    portfolio = load_user_portfolio(user_id)
    
    if not portfolio:
        return None
    
    response_patterns = portfolio.get("rules", {}).get("response_patterns", {})
    
    if not response_patterns:
        return None

    for pattern, response in response_patterns.items():
        if pattern == "ends_with_quoi":
            # Normaliser le message: enlever la ponctuation finale et les espaces
            normalized_message = re.sub(r'[\s\?\.!]+$', '', user_message).lower()
            if normalized_message.endswith("quoi"):
                return response
        elif pattern == "contains_quoi":
            if "quoi" in user_message.lower():
                return response
        elif pattern == "exact_quoi":
            if user_message.strip().lower() == "quoi":
                return response
        else:
            # Pattern regex personnalis√©
            try:
                if re.search(pattern, user_message, re.IGNORECASE):
                    return response
            except re.error:
                # Pattern invalide, ignorer
                continue
    
    return None

def reflective_thinking_process(user_message: str, user_id: str, team_id: str) -> Dict:
    """Processus de r√©flexion multi-√©tapes avant de r√©pondre avec portfolio utilisateur."""
    thinking_steps = []
    
    # √âtape 1: Analyse du message
    thinking_steps.append(f"üß† √âTAPE 1 - Analyse du message: '{user_message}'")
    
    # √âtape 2: V√©rification du portfolio utilisateur
    thinking_steps.append("üîç √âTAPE 2 - V√©rification du portfolio utilisateur...")
    portfolio = load_user_portfolio(user_id)
    
    if portfolio:
        thinking_steps.append(f"üìã Portfolio trouv√© pour {user_id}")
        
        # V√©rifier les r√®gles de r√©ponse
        rule_response = check_user_portfolio_rules(user_message, user_id)
        if rule_response:
            thinking_steps.append(f"‚úÖ R√®gle portfolio d√©clench√©e: {rule_response}")
            return {
                "response": rule_response,
                "thinking_steps": thinking_steps,
                "rule_triggered": True,
                "confidence": 1.0,
                "source": "portfolio"
            }
        else:
            thinking_steps.append("‚ÑπÔ∏è Aucune r√®gle portfolio d√©clench√©e")
    else:
        thinking_steps.append("‚ö†Ô∏è Portfolio utilisateur non trouv√©")
    
    # √âtape 3: Analyse s√©mantique et d√©tection de patterns
    thinking_steps.append("üîç √âTAPE 3 - Analyse s√©mantique du message...")
    
    patterns_detected = []
    if user_message.strip().lower().endswith("quoi"):
        patterns_detected.append("se_termine_par_quoi")
    if "quoi" in user_message.lower():
        patterns_detected.append("contient_quoi")
    if user_message.strip().lower() == "quoi":
        patterns_detected.append("exactement_quoi")
    
    # D√©tecter les r√©f√©rences temporelles pour les rappels
    time_references = detect_time_references(user_message)
    if time_references:
        thinking_steps.append(f"‚è∞ R√©f√©rences temporelles d√©tect√©es: {len(time_references)}")
        patterns_detected.append("contient_references_temps")
    
    if patterns_detected:
        thinking_steps.append(f"üéØ Patterns d√©tect√©s: {patterns_detected}")
    
    # √âtape 4: V√©rifier les rappels √† venir
    thinking_steps.append("üîî √âTAPE 4 - V√©rification des rappels √† venir...")
    upcoming_reminders = check_upcoming_reminders(team_id, hours_ahead=24)
    if upcoming_reminders:
        thinking_steps.append(f"‚è∞ {len(upcoming_reminders)} rappels √† venir d√©tect√©s")
        patterns_detected.append("rappels_a_venir")
    
    # √âtape 5: D√©terminer la strat√©gie de r√©ponse
    thinking_steps.append("üéØ √âTAPE 5 - Strat√©gie de r√©ponse d√©termin√©e")
    
    search_terms = ["r√®gle", "pattern", "r√©ponse"]
    if time_references:
        search_terms.extend(["rappel", "√©v√©nement", "temps"])
    
    return {
        "response": None,
        "thinking_steps": thinking_steps,
        "rule_triggered": False,
        "patterns_detected": patterns_detected,
        "time_references": time_references,
        "upcoming_reminders": upcoming_reminders,
        "needs_memory_search": True,
        "search_terms": search_terms,
        "portfolio_available": bool(portfolio)
    }

# Dictionnaire global pour mapper les noms d'outils aux fonctions
available_tools = {
    "add_memory": add_memory,
    "search_memories": search_memories,
    "delete_memory": delete_memory,
    "list_memories": list_memories,
    "get_team_insights": get_team_insights,
    "record_conversation_message": record_conversation_message,
    "get_conversation_summary": get_conversation_summary,
    "list_team_conversations": list_team_conversations,
    "generate_conversation_insights": generate_conversation_insights,
    "end_chat_and_summarize": end_chat_and_summarize,
    "search_past_conversations": search_past_conversations,
    "get_full_transcript": get_full_transcript,
    "search_in_transcript": search_in_transcript,
    "get_conversation_transcript": get_conversation_transcript,
    "list_available_transcripts": list_available_transcripts,
    "search_archived_conversations": search_archived_conversations,
    "multi_query_search": multi_query_search,
    "expand_search_keywords": expand_search_keywords,
    "add_user_rule": add_user_rule,
    # Nouveaux outils portfolio et rappels
    "update_user_portfolio": update_user_portfolio,
    "get_user_portfolio_summary": get_user_portfolio_summary,
    "add_reminder": add_reminder,
    "check_upcoming_reminders": check_upcoming_reminders_tool,
    "list_reminders": list_reminders,
    "delete_reminder": delete_reminder,
    "delete_all_reminders": delete_all_reminders,
}

# --- Fin des nouvelles fonctionnalit√©s ---


if __name__ == "__main__":
    if not IS_LAMBDA:
        print("üéØ D√©marrage du serveur MCP Collective Brain...")
    
    # Archivage automatique des conversations existantes
    auto_archive_existing_conversations()
    
    # Initialisation paresseuse
    mcp = initialize_mcp()
    
    if mcp:
        if not IS_LAMBDA:
            print("üöÄ Serveur MCP Collective Brain d√©marr√© - pr√™t √† recevoir des requ√™tes")
        mcp.run(transport="streamable-http")
    else:
        if not IS_LAMBDA:
            print("‚ùå Impossible de d√©marrer le serveur MCP")
